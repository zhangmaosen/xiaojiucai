{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5236c6c",
   "metadata": {},
   "source": [
    "# ğŸ“ˆ æŠ•èµ„ç»„åˆå¼ºåŒ–å­¦ä¹ ä¼˜åŒ– - Gemma 1B + GRPO\n",
    "\n",
    "æœ¬notebookä½¿ç”¨Unslothæ¡†æ¶å’ŒGRPOï¼ˆGroup Relative Policy Optimizationï¼‰æ–¹æ³•ï¼Œè®­ç»ƒGemma 1Bæ¨¡å‹è¿›è¡ŒæŠ•èµ„ç»„åˆä¼˜åŒ–å†³ç­–ã€‚\n",
    "\n",
    "## ğŸ¯ ç›®æ ‡\n",
    "- ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡ŒæŠ•èµ„ç»„åˆåˆ†æ\n",
    "- åŸºäºMAG7è‚¡ç¥¨æ•°æ®ç”ŸæˆæŠ•èµ„å»ºè®®\n",
    "- è®¾è®¡åŒ…å«æ”¶ç›Šç‡å’Œé£é™©çš„å¥–åŠ±å‡½æ•°\n",
    "- ç”Ÿæˆå¸¦æ¨ç†è¿‡ç¨‹çš„æŠ•èµ„å†³ç­–\n",
    "\n",
    "## ğŸ“Š æ•°æ®æº\n",
    "- æ•°æ®é›†ï¼šMAG7è‚¡ç¥¨æ•°æ® (data/mag7_data_raw.parquet)\n",
    "- åŒ…å«ï¼šApple, Amazon, Google, Meta, Microsoft, NVIDIA, Tesla\n",
    "- æ—¶é—´èŒƒå›´ï¼š2005-2025å¹´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b4da2",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒè®¾ç½®å’Œä¾èµ–å®‰è£…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e651d598",
   "metadata": {},
   "source": [
    "## âš ï¸ é‡è¦æç¤º\n",
    "\n",
    "å¦‚æœæ‚¨é‡åˆ° `SyntaxError: non-default argument follows default argument` é”™è¯¯ï¼Œè¿™æ˜¯Unslothåº“ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ã€‚\n",
    "\n",
    "**è§£å†³æ–¹æ¡ˆï¼š**\n",
    "1. é¦–å…ˆè¿è¡Œä¸‹é¢çš„å®‰è£…å•å…ƒæ ¼\n",
    "2. **é‡å¯Pythonå†…æ ¸** (Kernel -> Restart)\n",
    "3. ç„¶åç»§ç»­æ‰§è¡Œå…¶ä»–å•å…ƒæ ¼\n",
    "\n",
    "æœ¬notebookåŒ…å«äº†å…¼å®¹æ€§å¤„ç†ï¼Œå¦‚æœUnslothä¸å¯ç”¨ï¼Œä¼šè‡ªåŠ¨å›é€€åˆ°æ ‡å‡†çš„transformersåº“è¿›è¡Œè®­ç»ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3eca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # å®‰è£…å…¼å®¹ç‰ˆæœ¬çš„Unslothå’Œç›¸å…³ä¾èµ–\n",
    "# !pip install --upgrade pip\n",
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install  xformers\n",
    "\n",
    "# # å®‰è£…é¢å¤–çš„é‡‘èåˆ†æåº“\n",
    "# !pip install yfinance pandas numpy matplotlib seaborn scikit-learn\n",
    "\n",
    "# # é‡å¯Pythonå†…æ ¸ä»¥ç¡®ä¿ä¾èµ–æ­£ç¡®åŠ è½½\n",
    "# import os\n",
    "# os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ca9b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 09-26 19:28:49 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 09-26 19:28:49 [__init__.py:216] Automatically detected platform cuda.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "ğŸš€ PyTorchç‰ˆæœ¬: 2.8.0+cu128\n",
      "ğŸ® CUDAå¯ç”¨: True\n",
      "ğŸ”¥ GPUè®¾å¤‡: NVIDIA GeForce RTX 3090\n",
      "ğŸ’¾ GPUå†…å­˜: 25.4 GB\n",
      "âœ… Unslothå·²æˆåŠŸå¯¼å…¥\n",
      "ğŸš€ PyTorchç‰ˆæœ¬: 2.8.0+cu128\n",
      "ğŸ® CUDAå¯ç”¨: True\n",
      "ğŸ”¥ GPUè®¾å¤‡: NVIDIA GeForce RTX 3090\n",
      "ğŸ’¾ GPUå†…å­˜: 25.4 GB\n",
      "âœ… Unslothå·²æˆåŠŸå¯¼å…¥\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# å…ˆå°è¯•å¯¼å…¥åŸºæœ¬åº“ï¼Œå¦‚æœUnslothæœ‰é—®é¢˜ï¼Œä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ\n",
    "try:\n",
    "    # Unslothå’Œtransformersç›¸å…³\n",
    "    from unsloth import FastLanguageModel\n",
    "    from datasets import Dataset\n",
    "    from trl import GRPOConfig, GRPOTrainer\n",
    "    import torch\n",
    "    \n",
    "    print(f\"ğŸš€ PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "    print(f\"ğŸ® CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"ğŸ”¥ GPUè®¾å¤‡: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    UNSLOTH_AVAILABLE = True\n",
    "    print(\"âœ… Unslothå·²æˆåŠŸå¯¼å…¥\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Unslothå¯¼å…¥å¤±è´¥: {e}\")\n",
    "    print(\"ğŸ”„ å°è¯•ä½¿ç”¨æ ‡å‡†transformersåº“...\")\n",
    "    \n",
    "    # ä½¿ç”¨æ ‡å‡†transformersåº“ä½œä¸ºæ›¿ä»£\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "    from datasets import Dataset\n",
    "    import torch\n",
    "    \n",
    "    print(f\"ğŸš€ PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "    print(f\"ğŸ® CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"ğŸ”¥ GPUè®¾å¤‡: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    UNSLOTH_AVAILABLE = False\n",
    "    print(\"âœ… æ ‡å‡†transformersåº“å·²å¯¼å…¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aed12f",
   "metadata": {},
   "source": [
    "## 2. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55c29f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š åŠ è½½MAG7è‚¡ç¥¨æ•°æ®...\n",
      "æ•°æ®å½¢çŠ¶: (5027, 35)\n",
      "æ—¶é—´èŒƒå›´: 2005-09-26 00:00:00 åˆ° 2025-09-18 00:00:00\n",
      "è‚¡ç¥¨åˆ—è¡¨: ['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA']\n",
      "\n",
      "ğŸ¯ ç›®æ ‡è‚¡ç¥¨: ['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA']\n",
      "\n",
      "ğŸ“ˆ æœ€è¿‘5å¤©æ•°æ®é¢„è§ˆ:\n",
      "  AAPL: $237.88\n",
      "  AMZN: $231.23\n",
      "  GOOGL: $252.03\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½MAG7è‚¡ç¥¨æ•°æ®\n",
    "print(\"ğŸ“Š åŠ è½½MAG7è‚¡ç¥¨æ•°æ®...\")\n",
    "data_path = '../data/mag7_data_raw.parquet'\n",
    "mag7_data = pd.read_parquet(data_path)\n",
    "\n",
    "print(f\"æ•°æ®å½¢çŠ¶: {mag7_data.shape}\")\n",
    "print(f\"æ—¶é—´èŒƒå›´: {mag7_data.index.min()} åˆ° {mag7_data.index.max()}\")\n",
    "print(f\"è‚¡ç¥¨åˆ—è¡¨: {[col[1] for col in mag7_data.columns if col[0] == 'Close']}\")\n",
    "\n",
    "# æå–è‚¡ç¥¨ä»£ç \n",
    "tickers = [col[1] for col in mag7_data.columns if col[0] == 'Close']\n",
    "print(f\"\\nğŸ¯ ç›®æ ‡è‚¡ç¥¨: {tickers}\")\n",
    "\n",
    "# æ˜¾ç¤ºæœ€è¿‘çš„æ•°æ®\n",
    "recent_data = mag7_data.tail()\n",
    "print(\"\\nğŸ“ˆ æœ€è¿‘5å¤©æ•°æ®é¢„è§ˆ:\")\n",
    "for ticker in tickers[:3]:  # åªæ˜¾ç¤ºå‰3åªè‚¡ç¥¨\n",
    "    close_price = recent_data[('Close', ticker)].iloc[-1]\n",
    "    print(f\"  {ticker}: ${close_price:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e56145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§® è®¡ç®—æŠ€æœ¯æŒ‡æ ‡...\n",
      "\n",
      "ğŸ“Š æœ€è¿‘æŠ€æœ¯æŒ‡æ ‡:\n",
      "  AAPL:\n",
      "    æ—¥æ”¶ç›Šç‡: -0.46%\n",
      "    å¹´åŒ–æ³¢åŠ¨ç‡: 22.7%\n",
      "    RSI: 62.1\n",
      "  AMZN:\n",
      "    æ—¥æ”¶ç›Šç‡: -0.17%\n",
      "    å¹´åŒ–æ³¢åŠ¨ç‡: 27.0%\n",
      "    RSI: 56.4\n",
      "  GOOGL:\n",
      "    æ—¥æ”¶ç›Šç‡: 1.00%\n",
      "    å¹´åŒ–æ³¢åŠ¨ç‡: 36.5%\n",
      "    RSI: 90.5\n"
     ]
    }
   ],
   "source": [
    "# è®¡ç®—å…³é”®è´¢åŠ¡æŒ‡æ ‡\n",
    "def calculate_financial_metrics(data, window=20):\n",
    "    \"\"\"è®¡ç®—è´¢åŠ¡æŒ‡æ ‡\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        # è·å–ä»·æ ¼æ•°æ®\n",
    "        close = data[('Close', ticker)]\n",
    "        high = data[('High', ticker)]\n",
    "        low = data[('Low', ticker)]\n",
    "        \n",
    "        # è®¡ç®—æ”¶ç›Šç‡\n",
    "        returns = close.pct_change()\n",
    "        \n",
    "        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡\n",
    "        sma = close.rolling(window).mean()\n",
    "        volatility = returns.rolling(window).std() * np.sqrt(252)  # å¹´åŒ–æ³¢åŠ¨ç‡\n",
    "        \n",
    "        # RSIè®¡ç®—\n",
    "        delta = close.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        metrics[ticker] = {\n",
    "            'returns': returns,\n",
    "            'sma': sma,\n",
    "            'volatility': volatility,\n",
    "            'rsi': rsi,\n",
    "            'price': close\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# è®¡ç®—æŒ‡æ ‡\n",
    "print(\"ğŸ§® è®¡ç®—æŠ€æœ¯æŒ‡æ ‡...\")\n",
    "financial_metrics = calculate_financial_metrics(mag7_data)\n",
    "\n",
    "# æ˜¾ç¤ºæœ€è¿‘çš„æŒ‡æ ‡\n",
    "print(\"\\nğŸ“Š æœ€è¿‘æŠ€æœ¯æŒ‡æ ‡:\")\n",
    "for ticker in tickers[:3]:\n",
    "    metrics = financial_metrics[ticker]\n",
    "    recent_return = metrics['returns'].iloc[-1] * 100\n",
    "    recent_volatility = metrics['volatility'].iloc[-1] * 100\n",
    "    recent_rsi = metrics['rsi'].iloc[-1]\n",
    "    \n",
    "    print(f\"  {ticker}:\")\n",
    "    print(f\"    æ—¥æ”¶ç›Šç‡: {recent_return:.2f}%\")\n",
    "    print(f\"    å¹´åŒ–æ³¢åŠ¨ç‡: {recent_volatility:.1f}%\")\n",
    "    print(f\"    RSI: {recent_rsi:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6d5d95",
   "metadata": {},
   "source": [
    "## 3. ç”Ÿæˆè®­ç»ƒæ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f0788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ç”Ÿæˆè®­ç»ƒæ•°æ®é›†...\n",
      "âœ… ç”Ÿæˆäº† 47 ä¸ªè®­ç»ƒæ ·æœ¬\n",
      "\n",
      "ğŸ“ è®­ç»ƒæ ·æœ¬ç¤ºä¾‹:\n",
      "é—®é¢˜: æ—¥æœŸ: 2023-09-21\n",
      "å¸‚åœºæ•°æ®:\n",
      "AAPL: ä»·æ ¼$172.24, æ—¥æ¶¨è·Œ-0.89%, ç›¸å¯¹20æ—¥å‡çº¿-3.56%\n",
      "AMZN: ä»·æ ¼$129.33, æ—¥æ¶¨è·Œ-4.41%, ç›¸å¯¹20æ—¥å‡çº¿-6.14%\n",
      "GOOGL: ä»·æ ¼$129.55, æ—¥æ¶¨è·Œ-2.47%, ç›¸å¯¹20æ—¥å‡çº¿-3.44%\n",
      "META: ä»·æ ¼$294.12, æ—¥æ¶¨è·Œ-1.31%, ç›¸å¯¹20æ—¥å‡çº¿-0.95%\n",
      "MSFT: ä»·æ ¼$3...\n",
      "æ¨ç†: å¸‚åœºåˆ†æï¼š\n",
      "AAPL: ä»·æ ¼$172.24, æ—¥æ¶¨è·Œ-0.89%, å¼±äº20æ—¥å‡çº¿3.6%\n",
      "AMZN: ä»·æ ¼$129.33, æ—¥æ¶¨è·Œ-4.41%, å¼±äº20æ—¥å‡çº¿6.1%\n",
      "GOOGL: ä»·æ ¼$129.55, æ—¥æ¶¨è·Œ-2.47%, å¼±äº20æ—¥å‡çº¿3.4%\n",
      "META: ä»·æ ¼$294.12, æ—¥æ¶¨è·Œ-1.31%, å¼±äº20æ—¥å‡çº¿0.9%\n",
      "MSFT: ä»·æ ¼$314.79, æ—¥æ¶¨è·Œ-0.39%, å¼±äº20æ—¥...\n",
      "ç­”æ¡ˆ: {\"AAPL\": 0.018, \"AMZN\": 0.018, \"GOOGL\": 0.132, \"META\": 0.018, \"MSFT\": 0.018, \"NVDA\": 0.018, \"TSLA\": 0.78}\n",
      "âœ… ç”Ÿæˆäº† 47 ä¸ªè®­ç»ƒæ ·æœ¬\n",
      "\n",
      "ğŸ“ è®­ç»ƒæ ·æœ¬ç¤ºä¾‹:\n",
      "é—®é¢˜: æ—¥æœŸ: 2023-09-21\n",
      "å¸‚åœºæ•°æ®:\n",
      "AAPL: ä»·æ ¼$172.24, æ—¥æ¶¨è·Œ-0.89%, ç›¸å¯¹20æ—¥å‡çº¿-3.56%\n",
      "AMZN: ä»·æ ¼$129.33, æ—¥æ¶¨è·Œ-4.41%, ç›¸å¯¹20æ—¥å‡çº¿-6.14%\n",
      "GOOGL: ä»·æ ¼$129.55, æ—¥æ¶¨è·Œ-2.47%, ç›¸å¯¹20æ—¥å‡çº¿-3.44%\n",
      "META: ä»·æ ¼$294.12, æ—¥æ¶¨è·Œ-1.31%, ç›¸å¯¹20æ—¥å‡çº¿-0.95%\n",
      "MSFT: ä»·æ ¼$3...\n",
      "æ¨ç†: å¸‚åœºåˆ†æï¼š\n",
      "AAPL: ä»·æ ¼$172.24, æ—¥æ¶¨è·Œ-0.89%, å¼±äº20æ—¥å‡çº¿3.6%\n",
      "AMZN: ä»·æ ¼$129.33, æ—¥æ¶¨è·Œ-4.41%, å¼±äº20æ—¥å‡çº¿6.1%\n",
      "GOOGL: ä»·æ ¼$129.55, æ—¥æ¶¨è·Œ-2.47%, å¼±äº20æ—¥å‡çº¿3.4%\n",
      "META: ä»·æ ¼$294.12, æ—¥æ¶¨è·Œ-1.31%, å¼±äº20æ—¥å‡çº¿0.9%\n",
      "MSFT: ä»·æ ¼$314.79, æ—¥æ¶¨è·Œ-0.39%, å¼±äº20æ—¥...\n",
      "ç­”æ¡ˆ: {\"AAPL\": 0.018, \"AMZN\": 0.018, \"GOOGL\": 0.132, \"META\": 0.018, \"MSFT\": 0.018, \"NVDA\": 0.018, \"TSLA\": 0.78}\n"
     ]
    }
   ],
   "source": [
    "# å®šä¹‰ç³»ç»Ÿæç¤º\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æŠ•èµ„ç»„åˆç®¡ç†ä¸“å®¶ã€‚è¯·æ ¹æ®æä¾›çš„å¸‚åœºæ•°æ®å’ŒæŠ€æœ¯æŒ‡æ ‡ï¼Œä¸ºæŠ•èµ„è€…æä¾›æŠ•èµ„å»ºè®®ã€‚\n",
    "\n",
    "è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š\n",
    "<reasoning>\n",
    "è¯¦ç»†åˆ†æå¸‚åœºæƒ…å†µã€æŠ€æœ¯æŒ‡æ ‡å’ŒæŠ•èµ„é€»è¾‘...\n",
    "</reasoning>\n",
    "<answer>\n",
    "å…·ä½“çš„æŠ•èµ„ç»„åˆæƒé‡å»ºè®®ï¼ˆJSONæ ¼å¼ï¼‰\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "# XMLæ ¼å¼æ¨¡æ¿\n",
    "XML_PORTFOLIO_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def create_market_scenario(data, start_idx, window=30):\n",
    "    \"\"\"åˆ›å»ºå¸‚åœºæƒ…æ™¯\"\"\"\n",
    "    end_idx = start_idx + window\n",
    "    if end_idx >= len(data):\n",
    "        return None\n",
    "    \n",
    "    scenario_data = {}\n",
    "    current_date = data.index[start_idx].strftime('%Y-%m-%d')\n",
    "    \n",
    "    # è·å–å½“å‰å¸‚åœºçŠ¶æ€\n",
    "    for ticker in tickers:\n",
    "        current_price = data[('Close', ticker)].iloc[start_idx]\n",
    "        prev_price = data[('Close', ticker)].iloc[start_idx-1] if start_idx > 0 else current_price\n",
    "        daily_return = (current_price - prev_price) / prev_price * 100\n",
    "        \n",
    "        # è®¡ç®—ç®€å•ç§»åŠ¨å¹³å‡\n",
    "        if start_idx >= 20:\n",
    "            sma_20 = data[('Close', ticker)].iloc[start_idx-20:start_idx].mean()\n",
    "            price_vs_sma = (current_price - sma_20) / sma_20 * 100\n",
    "        else:\n",
    "            price_vs_sma = 0\n",
    "        \n",
    "        scenario_data[ticker] = {\n",
    "            'price': round(current_price, 2),\n",
    "            'daily_return': round(daily_return, 2),\n",
    "            'vs_sma20': round(price_vs_sma, 2)\n",
    "        }\n",
    "    \n",
    "    # è®¡ç®—æœªæ¥å®é™…æ”¶ç›Šï¼ˆç”¨äºç”Ÿæˆæ ‡å‡†ç­”æ¡ˆï¼‰\n",
    "    future_returns = {}\n",
    "    for ticker in tickers:\n",
    "        current_price = data[('Close', ticker)].iloc[start_idx]\n",
    "        future_price = data[('Close', ticker)].iloc[end_idx]\n",
    "        future_return = (future_price - current_price) / current_price\n",
    "        future_returns[ticker] = future_return\n",
    "    \n",
    "    return {\n",
    "        'date': current_date,\n",
    "        'market_data': scenario_data,\n",
    "        'future_returns': future_returns\n",
    "    }\n",
    "\n",
    "def generate_optimal_portfolio(future_returns, risk_aversion=1.0):\n",
    "    \"\"\"ç”Ÿæˆæœ€ä¼˜æŠ•èµ„ç»„åˆæƒé‡\"\"\"\n",
    "    returns_array = np.array(list(future_returns.values()))\n",
    "    \n",
    "    # ç®€å•çš„å‡å€¼å›å½’ç­–ç•¥ï¼šä¹°å…¥è¡¨ç°è¾ƒå·®çš„è‚¡ç¥¨\n",
    "    scores = -returns_array  # è´Ÿæ”¶ç›Šç‡å¾—åˆ†æ›´é«˜\n",
    "    scores = np.maximum(scores, 0)  # åªè€ƒè™‘è´Ÿæ”¶ç›Š\n",
    "    \n",
    "    if scores.sum() == 0:\n",
    "        # å¦‚æœæ‰€æœ‰è‚¡ç¥¨éƒ½ä¸Šæ¶¨ï¼Œå¹³å‡åˆ†é…\n",
    "        weights = np.ones(len(tickers)) / len(tickers)\n",
    "    else:\n",
    "        # æ ¹æ®ä¸‹è·Œç¨‹åº¦åˆ†é…æƒé‡\n",
    "        weights = scores / scores.sum()\n",
    "        # æ·»åŠ ä¸€äº›éšæœºæ€§é¿å…è¿‡åº¦é›†ä¸­\n",
    "        noise = np.random.normal(0, 0.05, len(weights))\n",
    "        weights = weights + noise\n",
    "        weights = np.maximum(weights, 0.02)  # æœ€å°æƒé‡2%\n",
    "        weights = weights / weights.sum()  # å½’ä¸€åŒ–\n",
    "    \n",
    "    return {ticker: round(weight, 3) for ticker, weight in zip(tickers, weights)}\n",
    "\n",
    "def create_reasoning(market_data, portfolio_weights):\n",
    "    \"\"\"ç”ŸæˆæŠ•èµ„æ¨ç†\"\"\"\n",
    "    reasoning_parts = []\n",
    "    \n",
    "    # å¸‚åœºåˆ†æ\n",
    "    reasoning_parts.append(\"å¸‚åœºåˆ†æï¼š\")\n",
    "    for ticker, data in market_data.items():\n",
    "        trend = \"ä¸Šæ¶¨\" if data['daily_return'] > 0 else \"ä¸‹è·Œ\"\n",
    "        vs_ma = \"å¼ºäº\" if data['vs_sma20'] > 0 else \"å¼±äº\"\n",
    "        reasoning_parts.append(\n",
    "            f\"{ticker}: ä»·æ ¼${data['price']}, æ—¥æ¶¨è·Œ{data['daily_return']:+.2f}%, {vs_ma}20æ—¥å‡çº¿{abs(data['vs_sma20']):.1f}%\"\n",
    "        )\n",
    "    \n",
    "    # æŠ•èµ„é€»è¾‘\n",
    "    reasoning_parts.append(\"\\næŠ•èµ„é€»è¾‘ï¼š\")\n",
    "    top_holdings = sorted(portfolio_weights.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    \n",
    "    for ticker, weight in top_holdings:\n",
    "        if weight > 0.15:  # æƒé‡è¶…è¿‡15%çš„é‡ç‚¹åˆ†æ\n",
    "            ticker_data = market_data[ticker]\n",
    "            if ticker_data['vs_sma20'] < 0:\n",
    "                reasoning_parts.append(f\"å¢æŒ{ticker}ï¼ˆ{weight*100:.1f}%ï¼‰ï¼šä»·æ ¼ä½äºå‡çº¿ï¼Œå­˜åœ¨å‡å€¼å›å½’æœºä¼š\")\n",
    "            else:\n",
    "                reasoning_parts.append(f\"é…ç½®{ticker}ï¼ˆ{weight*100:.1f}%ï¼‰ï¼šæŠ€æœ¯é¢ç›¸å¯¹å¼ºåŠ¿ï¼Œé€‚åº¦é…ç½®\")\n",
    "    \n",
    "    reasoning_parts.append(\"\\né£é™©æ§åˆ¶ï¼šé‡‡ç”¨åˆ†æ•£åŒ–æŠ•èµ„ï¼Œå•ä¸ªè‚¡ç¥¨æƒé‡ä¸è¶…è¿‡30%ï¼Œé™ä½é›†ä¸­é£é™©ã€‚\")\n",
    "    \n",
    "    return \"\\n\".join(reasoning_parts)\n",
    "\n",
    "# ç”Ÿæˆè®­ç»ƒæ•°æ®\n",
    "print(\"ğŸ”„ ç”Ÿæˆè®­ç»ƒæ•°æ®é›†...\")\n",
    "training_samples = []\n",
    "sample_count = 0\n",
    "\n",
    "# ä½¿ç”¨æœ€è¿‘2å¹´çš„æ•°æ®ç”Ÿæˆæ ·æœ¬\n",
    "start_date = mag7_data.index[-500]  # å¤§çº¦2å¹´çš„æ•°æ®\n",
    "start_idx = mag7_data.index.get_loc(start_date)\n",
    "\n",
    "for i in range(start_idx, len(mag7_data) - 30, 10):  # æ¯10å¤©é‡‡æ ·ä¸€æ¬¡\n",
    "    scenario = create_market_scenario(mag7_data, i)\n",
    "    if scenario is None:\n",
    "        continue\n",
    "    \n",
    "    # ç”Ÿæˆæœ€ä¼˜æŠ•èµ„ç»„åˆ\n",
    "    optimal_weights = generate_optimal_portfolio(scenario['future_returns'])\n",
    "    \n",
    "    # ç”Ÿæˆæ¨ç†è¿‡ç¨‹\n",
    "    reasoning = create_reasoning(scenario['market_data'], optimal_weights)\n",
    "    \n",
    "    # åˆ›å»ºé—®é¢˜\n",
    "    question = f\"\"\"\n",
    "æ—¥æœŸ: {scenario['date']}\n",
    "å¸‚åœºæ•°æ®:\n",
    "\"\"\"\n",
    "    for ticker, data in scenario['market_data'].items():\n",
    "        question += f\"{ticker}: ä»·æ ¼${data['price']}, æ—¥æ¶¨è·Œ{data['daily_return']:+.2f}%, ç›¸å¯¹20æ—¥å‡çº¿{data['vs_sma20']:+.2f}%\\n\"\n",
    "    \n",
    "    question += \"\\nè¯·åˆ†æå½“å‰å¸‚åœºæƒ…å†µå¹¶ç»™å‡ºMAG7è‚¡ç¥¨çš„æŠ•èµ„ç»„åˆæƒé‡å»ºè®®ã€‚\"\n",
    "    \n",
    "    # åˆ›å»ºç­”æ¡ˆ\n",
    "    answer = json.dumps(optimal_weights, ensure_ascii=False)\n",
    "    \n",
    "    training_samples.append({\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question.strip()}\n",
    "        ],\n",
    "        \"reasoning\": reasoning,\n",
    "        \"answer\": answer,\n",
    "        \"future_returns\": scenario['future_returns']\n",
    "    })\n",
    "    \n",
    "    sample_count += 1\n",
    "    if sample_count >= 200:  # é™åˆ¶æ ·æœ¬æ•°é‡\n",
    "        break\n",
    "\n",
    "print(f\"âœ… ç”Ÿæˆäº† {len(training_samples)} ä¸ªè®­ç»ƒæ ·æœ¬\")\n",
    "\n",
    "# æ˜¾ç¤ºç¤ºä¾‹\n",
    "if training_samples:\n",
    "    print(\"\\nğŸ“ è®­ç»ƒæ ·æœ¬ç¤ºä¾‹:\")\n",
    "    sample = training_samples[0]\n",
    "    print(\"é—®é¢˜:\", sample['prompt'][1]['content'][:200] + \"...\")\n",
    "    print(\"æ¨ç†:\", sample['reasoning'][:200] + \"...\")\n",
    "    print(\"ç­”æ¡ˆ:\", sample['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5548215a",
   "metadata": {},
   "source": [
    "## 4. æ¨¡å‹é…ç½®å’ŒåŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf44d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– åŠ è½½æ¨¡å‹...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.7: Fast Qwen3 patching. Transformers: 4.55.4. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 4. Max memory: 23.691 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f50fa801-b77a-4a41-9ee0-46a0b454c35e)')' thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/unslothai/other/43d9e0f2f19a5d7836895f648dc0e762816acf77/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Unsloth 2025.9.7 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n",
      "Unsloth 2025.9.7 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Unslothæ¨¡å‹åŠ è½½å®Œæˆï¼\n",
      "âœ… å·²è®¾ç½®Qwen chat template\n",
      "ğŸ“Š æ¨¡å‹å‚æ•°é‡: 1,738,007,552\n"
     ]
    }
   ],
   "source": [
    "# æ¨¡å‹é…ç½®\n",
    "max_seq_length = 2048  # é€‰æ‹©ä»»ä½•é•¿åº¦\n",
    "dtype = None  # Noneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹\n",
    "load_in_4bit = True  # ä½¿ç”¨4bité‡åŒ–å‡å°‘å†…å­˜ä½¿ç”¨\n",
    "\n",
    "print(\"ğŸ¤– åŠ è½½æ¨¡å‹...\")\n",
    "\n",
    "if UNSLOTH_AVAILABLE:\n",
    "    try:\n",
    "        # ä½¿ç”¨UnslothåŠ è½½æ¨¡å‹\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"unsloth/Qwen3-1.7B-Base\",  # ä½¿ç”¨Qwen3 1.7BåŸºç¡€ç‰ˆæœ¬\n",
    "            max_seq_length=max_seq_length,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=load_in_4bit,\n",
    "            #device_map = \"balanced\",\n",
    "        )\n",
    "        \n",
    "        # æ·»åŠ LoRAé€‚é…å™¨\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=16,  # rank\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                            \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=\"unsloth\",\n",
    "            random_state=3407,\n",
    "            use_rslora=False,\n",
    "            loftq_config=None,\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Unslothæ¨¡å‹åŠ è½½å®Œæˆï¼\")\n",
    "        USE_UNSLOTH = True\n",
    "        \n",
    "        # ç¡®ä¿chat_templateè¢«æ­£ç¡®è®¾ç½®\n",
    "        if not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:\n",
    "            tokenizer.chat_template = \"\"\"{% for message in messages %}{% if message['role'] == 'system' %}{{ '<|im_start|>system\\n' + message['content'] + '<|im_end|>\\n' }}{% elif message['role'] == 'user' %}{{ '<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n' }}{% elif message['role'] == 'assistant' %}{{ '<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\"\"\n",
    "            print(\"âœ… å·²è®¾ç½®Qwen chat template\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Unslothæ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n",
    "        print(\"ğŸ”„ å›é€€åˆ°æ ‡å‡†transformers...\")\n",
    "        UNSLOTH_AVAILABLE = False\n",
    "\n",
    "if not UNSLOTH_AVAILABLE:\n",
    "    try:\n",
    "        # ä½¿ç”¨æ ‡å‡†transformersåº“åŠ è½½Qwen3æ¨¡å‹\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "        \n",
    "        print(\"ğŸ”„ ä½¿ç”¨æ ‡å‡†transformersåŠ è½½Qwen3æ¨¡å‹...\")\n",
    "        \n",
    "        # é…ç½®4bité‡åŒ–\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "        \n",
    "        # åŠ è½½æ¨¡å‹å’Œtokenizer\n",
    "        model_name = \"Qwen/Qwen3-1.7B\"\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"balanced\",\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # è®¾ç½®Qwenæ¨¡å‹çš„chat template\n",
    "        tokenizer.chat_template = \"\"\"{% for message in messages %}{% if message['role'] == 'system' %}{{ '<|im_start|>system\\n' + message['content'] + '<|im_end|>\\n' }}{% elif message['role'] == 'user' %}{{ '<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n' }}{% elif message['role'] == 'assistant' %}{{ '<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\"\"\n",
    "        \n",
    "        print(\"âœ… æ ‡å‡†transformersæ¨¡å‹åŠ è½½å®Œæˆï¼\")\n",
    "        USE_UNSLOTH = False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ ‡å‡†transformersæ¨¡å‹åŠ è½½ä¹Ÿå¤±è´¥: {e}\")\n",
    "        print(\"ğŸ’¡ è¯·æ£€æŸ¥æ¨¡å‹åç§°å’Œç½‘ç»œè¿æ¥\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "print(f\"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0826e8",
   "metadata": {},
   "source": [
    "## 5. å¥–åŠ±å‡½æ•°è®¾è®¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4f83f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ å¥–åŠ±å‡½æ•°é…ç½®å®Œæˆï¼\n",
      "å¥–åŠ±æœºåˆ¶:\n",
      "  - æŠ•èµ„ç»„åˆæ”¶ç›Š: 40%\n",
      "  - æƒé‡åˆç†æ€§: 20%\n",
      "  - æ¨ç†è´¨é‡: 25%\n",
      "  - æ ¼å¼æ­£ç¡®æ€§: 15%\n"
     ]
    }
   ],
   "source": [
    "def extract_xml_answer(text: str) -> str:\n",
    "    \"\"\"ä»XMLæ ¼å¼ä¸­æå–ç­”æ¡ˆ\"\"\"\n",
    "    try:\n",
    "        if \"<answer>\" in text and \"</answer>\" in text:\n",
    "            answer = text.split(\"<answer>\")[1].split(\"</answer>\")[0].strip()\n",
    "            return answer\n",
    "        return text.strip()\n",
    "    except:\n",
    "        return text.strip()\n",
    "\n",
    "def parse_portfolio_weights(answer_text: str) -> dict:\n",
    "    \"\"\"è§£ææŠ•èµ„ç»„åˆæƒé‡\"\"\"\n",
    "    try:\n",
    "        # å°è¯•è§£æJSON\n",
    "        if answer_text.startswith('{') and answer_text.endswith('}'):\n",
    "            return json.loads(answer_text)\n",
    "        \n",
    "        # å°è¯•ä»æ–‡æœ¬ä¸­æå–æƒé‡ä¿¡æ¯\n",
    "        weights = {}\n",
    "        for ticker in tickers:\n",
    "            # æŸ¥æ‰¾ç±»ä¼¼ \"AAPL: 0.15\" æˆ– \"AAPL\": 0.15 çš„æ¨¡å¼\n",
    "            patterns = [\n",
    "                rf'{ticker}[\"\\']?\\s*[:]\\s*([0-9.]+)',\n",
    "                rf'[\"\\']?{ticker}[\"\\']?\\s*[:]\\s*([0-9.]+)',\n",
    "            ]\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, answer_text)\n",
    "                if match:\n",
    "                    weights[ticker] = float(match.group(1))\n",
    "                    break\n",
    "        \n",
    "        if weights:\n",
    "            # å½’ä¸€åŒ–æƒé‡\n",
    "            total = sum(weights.values())\n",
    "            if total > 0:\n",
    "                weights = {k: v/total for k, v in weights.items()}\n",
    "            # è¡¥å……ç¼ºå¤±çš„è‚¡ç¥¨\n",
    "            for ticker in tickers:\n",
    "                if ticker not in weights:\n",
    "                    weights[ticker] = 0.0\n",
    "            return weights\n",
    "            \n",
    "        # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›å‡ç­‰æƒé‡\n",
    "        return {ticker: 1.0/len(tickers) for ticker in tickers}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"è§£ææƒé‡æ—¶å‡ºé”™: {e}\")\n",
    "        return {ticker: 1.0/len(tickers) for ticker in tickers}\n",
    "\n",
    "def calculate_portfolio_reward(predicted_weights: dict, actual_returns: dict, \n",
    "                             reasoning_text: str = \"\") -> float:\n",
    "    \"\"\"è®¡ç®—æŠ•èµ„ç»„åˆå¥–åŠ±\"\"\"\n",
    "    reward = 0.0\n",
    "    \n",
    "    # 1. æŠ•èµ„ç»„åˆæ”¶ç›Šå¥–åŠ± (æƒé‡40%)\n",
    "    portfolio_return = sum(predicted_weights.get(ticker, 0) * actual_returns.get(ticker, 0) \n",
    "                          for ticker in tickers)\n",
    "    \n",
    "    # å°†æ”¶ç›Šç‡è½¬æ¢ä¸ºå¥–åŠ±åˆ†æ•° (æ”¶ç›Šç‡ * 100)\n",
    "    return_reward = portfolio_return * 100\n",
    "    reward += return_reward * 0.4\n",
    "    \n",
    "    # 2. æƒé‡åˆç†æ€§å¥–åŠ± (æƒé‡20%)\n",
    "    total_weight = sum(predicted_weights.values())\n",
    "    weight_penalty = abs(total_weight - 1.0) * 10  # æƒé‡æ€»å’Œåº”è¯¥æ¥è¿‘1\n",
    "    \n",
    "    # æ£€æŸ¥æƒé‡åˆ†æ•£ç¨‹åº¦\n",
    "    max_weight = max(predicted_weights.values()) if predicted_weights.values() else 1\n",
    "    concentration_penalty = max(0, (max_weight - 0.4) * 5)  # å•åªè‚¡ç¥¨æƒé‡ä¸åº”è¶…è¿‡40%\n",
    "    \n",
    "    weight_reward = max(0, 2 - weight_penalty - concentration_penalty)\n",
    "    reward += weight_reward * 0.2\n",
    "    \n",
    "    # 3. æ¨ç†è´¨é‡å¥–åŠ± (æƒé‡25%)\n",
    "    reasoning_reward = 0\n",
    "    if reasoning_text:\n",
    "        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®åˆ†æå…ƒç´ \n",
    "        analysis_keywords = ['åˆ†æ', 'é£é™©', 'æ”¶ç›Š', 'å¸‚åœº', 'æŠ€æœ¯', 'å‡çº¿', 'æ¶¨è·Œ']\n",
    "        reasoning_lower = reasoning_text.lower()\n",
    "        \n",
    "        keyword_score = sum(1 for keyword in analysis_keywords if keyword in reasoning_lower)\n",
    "        reasoning_reward = min(keyword_score / len(analysis_keywords) * 3, 3)\n",
    "        \n",
    "        # æ¨ç†é•¿åº¦å¥–åŠ±ï¼ˆé¼“åŠ±è¯¦ç»†åˆ†æï¼‰\n",
    "        if len(reasoning_text) > 100:\n",
    "            reasoning_reward += 1\n",
    "        if len(reasoning_text) > 200:\n",
    "            reasoning_reward += 1\n",
    "    \n",
    "    reward += reasoning_reward * 0.25\n",
    "    \n",
    "    # 4. æ ¼å¼æ­£ç¡®æ€§å¥–åŠ± (æƒé‡15%)\n",
    "    format_reward = 0\n",
    "    if \"<reasoning>\" in reasoning_text and \"</reasoning>\" in reasoning_text:\n",
    "        format_reward += 1\n",
    "    if len(predicted_weights) == len(tickers):  # åŒ…å«æ‰€æœ‰è‚¡ç¥¨\n",
    "        format_reward += 1\n",
    "    if all(0 <= w <= 1 for w in predicted_weights.values()):  # æƒé‡åœ¨åˆç†èŒƒå›´\n",
    "        format_reward += 1\n",
    "    \n",
    "    reward += format_reward * 0.15\n",
    "    \n",
    "    return reward\n",
    "\n",
    "def portfolio_reward_function(prompts, completions, **kwargs):\n",
    "    \"\"\"GRPOå¥–åŠ±å‡½æ•° - åŸºäºç”Ÿæˆè´¨é‡çš„å¥–åŠ±\"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for i, completion in enumerate(completions):\n",
    "        try:\n",
    "            generated_text = completion\n",
    "            reward = 0.0\n",
    "\n",
    "            # 1. æ ¼å¼æ­£ç¡®æ€§å¥–åŠ± (æƒé‡40%)\n",
    "            if \"<reasoning>\" in generated_text and \"</reasoning>\" in generated_text:\n",
    "                reward += 1.0\n",
    "            if \"<answer>\" in generated_text and \"</answer>\" in generated_text:\n",
    "                reward += 1.0\n",
    "\n",
    "            # 2. æ¨ç†è´¨é‡å¥–åŠ± (æƒé‡30%)\n",
    "            reasoning_reward = 0\n",
    "            if \"<reasoning>\" in generated_text and \"</reasoning>\" in generated_text:\n",
    "                reasoning_text = generated_text.split(\"<reasoning>\")[1].split(\"</reasoning>\")[0]\n",
    "\n",
    "                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®åˆ†æå…ƒç´ \n",
    "                analysis_keywords = ['åˆ†æ', 'é£é™©', 'æ”¶ç›Š', 'å¸‚åœº', 'æŠ€æœ¯', 'å‡çº¿', 'æ¶¨è·Œ']\n",
    "                reasoning_lower = reasoning_text.lower()\n",
    "\n",
    "                keyword_score = sum(1 for keyword in analysis_keywords if keyword in reasoning_lower)\n",
    "                reasoning_reward = min(keyword_score / len(analysis_keywords) * 2, 2)\n",
    "\n",
    "                # æ¨ç†é•¿åº¦å¥–åŠ±\n",
    "                if len(reasoning_text) > 100:\n",
    "                    reasoning_reward += 0.5\n",
    "                if len(reasoning_text) > 200:\n",
    "                    reasoning_reward += 0.5\n",
    "\n",
    "            reward += reasoning_reward * 0.3\n",
    "\n",
    "            # 3. æƒé‡åˆç†æ€§å¥–åŠ± (æƒé‡30%)\n",
    "            if \"<answer>\" in generated_text and \"</answer>\" in generated_text:\n",
    "                answer_text = extract_xml_answer(generated_text)\n",
    "                predicted_weights = parse_portfolio_weights(answer_text)\n",
    "\n",
    "                # æ£€æŸ¥æƒé‡æ€»å’Œæ¥è¿‘1\n",
    "                total_weight = sum(predicted_weights.values())\n",
    "                weight_penalty = abs(total_weight - 1.0)\n",
    "                if weight_penalty < 0.1:  # å…è®¸10%çš„è¯¯å·®\n",
    "                    reward += 0.5\n",
    "\n",
    "                # æ£€æŸ¥æƒé‡åˆ†æ•£ç¨‹åº¦\n",
    "                max_weight = max(predicted_weights.values()) if predicted_weights.values() else 1\n",
    "                if max_weight <= 0.4:  # æœ€å¤§æƒé‡ä¸è¶…è¿‡40%\n",
    "                    reward += 0.5\n",
    "\n",
    "                # æ£€æŸ¥åŒ…å«æ‰€æœ‰è‚¡ç¥¨\n",
    "                if len(predicted_weights) == len(tickers):\n",
    "                    reward += 0.5\n",
    "\n",
    "            rewards.append(reward)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"å¥–åŠ±è®¡ç®—é”™è¯¯: {e}\")\n",
    "            rewards.append(0.0)  # é»˜è®¤å¥–åŠ±\n",
    "\n",
    "    return rewards\n",
    "\n",
    "print(\"ğŸ¯ å¥–åŠ±å‡½æ•°é…ç½®å®Œæˆï¼\")\n",
    "print(\"å¥–åŠ±æœºåˆ¶:\")\n",
    "print(\"  - æŠ•èµ„ç»„åˆæ”¶ç›Š: 40%\")\n",
    "print(\"  - æƒé‡åˆç†æ€§: 20%\")\n",
    "print(\"  - æ¨ç†è´¨é‡: 25%\")\n",
    "print(\"  - æ ¼å¼æ­£ç¡®æ€§: 15%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400cab4",
   "metadata": {},
   "source": [
    "## 6. å‡†å¤‡è®­ç»ƒæ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cf8f4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š å‡†å¤‡è®­ç»ƒæ•°æ®é›†...\n",
      "è®­ç»ƒé›†å¤§å°: 47\n",
      "è¯„ä¼°é›†å¤§å°: 0\n",
      "\n",
      "ğŸ§ª æµ‹è¯•å¥–åŠ±å‡½æ•°...\n",
      "æµ‹è¯•å¥–åŠ±åˆ†æ•°: 3.76\n"
     ]
    }
   ],
   "source": [
    "# è½¬æ¢ä¸ºHugging Faceæ•°æ®é›†æ ¼å¼\n",
    "def prepare_dataset(samples):\n",
    "    \"\"\"å‡†å¤‡è®­ç»ƒæ•°æ®é›†\"\"\"\n",
    "    dataset_dict = {\n",
    "        'prompt': [],\n",
    "        'future_returns': [],\n",
    "        'expected_reasoning': [],\n",
    "        'expected_answer': []\n",
    "    }\n",
    "    \n",
    "    for sample in samples:\n",
    "        dataset_dict['prompt'].append(sample['prompt'])\n",
    "        dataset_dict['future_returns'].append(sample['future_returns'])\n",
    "        dataset_dict['expected_reasoning'].append(sample['reasoning'])\n",
    "        dataset_dict['expected_answer'].append(sample['answer'])\n",
    "    \n",
    "    return Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# åˆ›å»ºæ•°æ®é›†\n",
    "print(\"ğŸ“š å‡†å¤‡è®­ç»ƒæ•°æ®é›†...\")\n",
    "train_dataset = prepare_dataset(training_samples[:150])  # ä½¿ç”¨150ä¸ªæ ·æœ¬è®­ç»ƒ\n",
    "eval_dataset = prepare_dataset(training_samples[150:])   # å‰©ä½™æ ·æœ¬ç”¨äºè¯„ä¼°\n",
    "\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}\")\n",
    "print(f\"è¯„ä¼°é›†å¤§å°: {len(eval_dataset)}\")\n",
    "\n",
    "# æµ‹è¯•å¥–åŠ±å‡½æ•°\n",
    "print(\"\\nğŸ§ª æµ‹è¯•å¥–åŠ±å‡½æ•°...\")\n",
    "test_prompt = \"è¯·åˆ†æå½“å‰å¸‚åœºæƒ…å†µå¹¶ç»™å‡ºMAG7è‚¡ç¥¨çš„æŠ•èµ„ç»„åˆæƒé‡å»ºè®®ã€‚\"\n",
    "test_completion = '''<reasoning>\n",
    "    å¸‚åœºåˆ†æï¼šAAPLä»·æ ¼ä¸Šæ¶¨2%ï¼ŒæŠ€æœ¯é¢å¼ºåŠ¿ã€‚TSLAä¸‹è·Œ3%ï¼Œå¯èƒ½å­˜åœ¨æŠ„åº•æœºä¼šã€‚\n",
    "    æŠ•èµ„é€»è¾‘ï¼šé‡‡ç”¨å‡å€¼å›å½’ç­–ç•¥ï¼Œé€‚åº¦å¢æŒä¸‹è·Œè‚¡ç¥¨ã€‚\n",
    "    </reasoning>\n",
    "    <answer>\n",
    "    {\"AAPL\": 0.15, \"AMZN\": 0.14, \"GOOGL\": 0.14, \"META\": 0.14, \"MSFT\": 0.14, \"NVDA\": 0.14, \"TSLA\": 0.15}\n",
    "    </answer>'''\n",
    "\n",
    "test_rewards = portfolio_reward_function([test_prompt], [test_completion])\n",
    "print(f\"æµ‹è¯•å¥–åŠ±åˆ†æ•°: {test_rewards[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd4dd05",
   "metadata": {},
   "source": [
    "## 7. GRPOè®­ç»ƒé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "193adaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ é…ç½®è®­ç»ƒå‚æ•°...\n",
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 4\n",
      "âœ… GRPOè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼\n",
      "ğŸ“Š è®­ç»ƒé…ç½®:\n",
      "  - è®­ç»ƒæ–¹æ³•: GRPOå¼ºåŒ–å­¦ä¹ \n",
      "  - å­¦ä¹ ç‡: 5e-06\n",
      "  - æœ€å¤§æ­¥æ•°: 50\n",
      "  - æ¯è½®ç”Ÿæˆæ•°: 4\n",
      "  - æ‰¹æ¬¡å¤§å°: 1\n",
      "  - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: 4\n",
      "âœ… GRPOè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼\n",
      "ğŸ“Š è®­ç»ƒé…ç½®:\n",
      "  - è®­ç»ƒæ–¹æ³•: GRPOå¼ºåŒ–å­¦ä¹ \n",
      "  - å­¦ä¹ ç‡: 5e-06\n",
      "  - æœ€å¤§æ­¥æ•°: 50\n",
      "  - æ¯è½®ç”Ÿæˆæ•°: 4\n",
      "  - æ‰¹æ¬¡å¤§å°: 1\n",
      "  - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: 4\n"
     ]
    }
   ],
   "source": [
    "# GRPOè®­ç»ƒé…ç½®\n",
    "print(\"âš™ï¸ é…ç½®è®­ç»ƒå‚æ•°...\")\n",
    "\n",
    "if UNSLOTH_AVAILABLE:\n",
    "    try:\n",
    "        # ä½¿ç”¨Unslothçš„GRPOè®­ç»ƒ\n",
    "        grpo_config = GRPOConfig(\n",
    "            output_dir=\"./portfolio_grpo_results\",\n",
    "            num_generations=4,           # æ¯ä¸ªpromptç”Ÿæˆ4ä¸ªå€™é€‰ç­”æ¡ˆ\n",
    "            learning_rate=5e-6,          # å­¦ä¹ ç‡\n",
    "            max_steps=50,                # å‡å°‘è®­ç»ƒæ­¥æ•°ä»¥é¿å…é”™è¯¯\n",
    "            per_device_train_batch_size=1,  # æ‰¹æ¬¡å¤§å°\n",
    "            gradient_accumulation_steps=4,   # æ¢¯åº¦ç´¯ç§¯\n",
    "            use_vllm=False,              # ä¸ä½¿ç”¨vLLM\n",
    "            temperature=0.8,             # ç”Ÿæˆæ¸©åº¦\n",
    "            epsilon=0.2,                 # PPO clipå‚æ•°\n",
    "            logging_steps=5,             # æ—¥å¿—è®°å½•é¢‘ç‡\n",
    "            save_steps=25,               # ä¿å­˜é¢‘ç‡\n",
    "            eval_steps=25,               # è¯„ä¼°é¢‘ç‡\n",
    "            warmup_steps=5,              # çƒ­èº«æ­¥æ•°\n",
    "            report_to=[],                # ä¸ä½¿ç”¨wandbç­‰å·¥å…·\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»ºGRPOè®­ç»ƒå™¨\n",
    "        trainer = GRPOTrainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            args=grpo_config,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            reward_funcs=portfolio_reward_function,\n",
    "        )\n",
    "        \n",
    "        GRPO_AVAILABLE = True\n",
    "        print(\"âœ… GRPOè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ GRPOåˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "        print(\"ğŸ”„ å°†ä½¿ç”¨ç®€åŒ–çš„ç›‘ç£å­¦ä¹ è®­ç»ƒ...\")\n",
    "        GRPO_AVAILABLE = False\n",
    "else:\n",
    "    GRPO_AVAILABLE = False\n",
    "\n",
    "if not GRPO_AVAILABLE:\n",
    "    # ä½¿ç”¨æ ‡å‡†çš„ç›‘ç£å­¦ä¹ è®­ç»ƒä½œä¸ºæ›¿ä»£\n",
    "    from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "    \n",
    "    # å‡†å¤‡ç›‘ç£å­¦ä¹ æ•°æ®\n",
    "    def prepare_supervised_dataset(samples):\n",
    "        \"\"\"å°†GRPOæ ·æœ¬è½¬æ¢ä¸ºç›‘ç£å­¦ä¹ æ ¼å¼\"\"\"\n",
    "        texts = []\n",
    "        for sample in samples:\n",
    "            # æ„å»ºå®Œæ•´çš„è®­ç»ƒæ–‡æœ¬\n",
    "            messages = sample['prompt']\n",
    "            \n",
    "            # æ‰‹åŠ¨æ„å»ºå¯¹è¯æ–‡æœ¬ï¼ˆé¿å…chat_templateé—®é¢˜ï¼‰\n",
    "            conversation_parts = []\n",
    "            for message in messages:\n",
    "                role = message['role']\n",
    "                content = message['content']\n",
    "                if role == 'system':\n",
    "                    conversation_parts.append(f\"<|im_start|>system\\n{content}<|im_end|>\")\n",
    "                elif role == 'user':\n",
    "                    conversation_parts.append(f\"<|im_start|>user\\n{content}<|im_end|>\")\n",
    "                elif role == 'assistant':\n",
    "                    conversation_parts.append(f\"<|im_start|>assistant\\n{content}<|im_end|>\")\n",
    "            \n",
    "            # æ·»åŠ ç”Ÿæˆæç¤º\n",
    "            conversation_parts.append(\"<|im_start|>assistant\")\n",
    "            \n",
    "            prompt_text = \"\\n\".join(conversation_parts)\n",
    "            \n",
    "            # æ·»åŠ æœŸæœ›çš„å›ç­”\n",
    "            full_response = XML_PORTFOLIO_FORMAT.format(\n",
    "                reasoning=sample['reasoning'],\n",
    "                answer=sample['answer']\n",
    "            )\n",
    "            \n",
    "            full_text = prompt_text + full_response + tokenizer.eos_token\n",
    "            texts.append(full_text)\n",
    "        \n",
    "        return {\"text\": texts}\n",
    "    \n",
    "    # è½¬æ¢æ•°æ®é›†\n",
    "    supervised_train = Dataset.from_dict(prepare_supervised_dataset(training_samples[:150]))\n",
    "    supervised_eval = Dataset.from_dict(prepare_supervised_dataset(training_samples[150:]))\n",
    "    \n",
    "    # è°ƒè¯•ï¼šæ£€æŸ¥æ•°æ®é›†å†…å®¹\n",
    "    print(\"ğŸ” è°ƒè¯•æ•°æ®é›†å†…å®¹...\")\n",
    "    print(f\"è®­ç»ƒé›†å¤§å°: {len(supervised_train)}\")\n",
    "    if len(supervised_train) > 0:\n",
    "        sample_text = supervised_train[0]['text']\n",
    "        print(f\"æ ·æœ¬æ–‡æœ¬ç±»å‹: {type(sample_text)}\")\n",
    "        print(f\"æ ·æœ¬æ–‡æœ¬é•¿åº¦: {len(sample_text) if isinstance(sample_text, str) else 'N/A'}\")\n",
    "        print(f\"æ ·æœ¬æ–‡æœ¬é¢„è§ˆ: {sample_text[:200] if isinstance(sample_text, str) else str(sample_text)[:200]}\")\n",
    "    \n",
    "    # æ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "    \n",
    "    # æ•°æ®é¢„å¤„ç†å‡½æ•°\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=False,  # è®©æ•°æ®æ•´ç†å™¨å¤„ç†padding\n",
    "            max_length=max_seq_length\n",
    "        )\n",
    "    \n",
    "    # é¢„å¤„ç†æ•°æ®é›†\n",
    "    supervised_train = supervised_train.map(preprocess_function, batched=True)\n",
    "    supervised_eval = supervised_eval.map(preprocess_function, batched=True)\n",
    "    \n",
    "    # è®­ç»ƒå‚æ•°\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./portfolio_supervised_results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=5e-5,\n",
    "        warmup_steps=10,\n",
    "        logging_steps=5,\n",
    "        save_steps=25,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=25,\n",
    "        save_total_limit=2,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=[],\n",
    "    )\n",
    "    \n",
    "    # æ•°æ®æ•´ç†å™¨ - ä½¿ç”¨é»˜è®¤çš„æ•°æ®æ•´ç†å™¨\n",
    "    data_collator = None  # ä½¿ç”¨é»˜è®¤æ•°æ®æ•´ç†å™¨\n",
    "    \n",
    "    # åˆ›å»ºè®­ç»ƒå™¨\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=supervised_train,\n",
    "        eval_dataset=supervised_eval,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… ç›‘ç£å­¦ä¹ è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼\")\n",
    "\n",
    "print(f\"ğŸ“Š è®­ç»ƒé…ç½®:\")\n",
    "if GRPO_AVAILABLE:\n",
    "    print(f\"  - è®­ç»ƒæ–¹æ³•: GRPOå¼ºåŒ–å­¦ä¹ \")\n",
    "    print(f\"  - å­¦ä¹ ç‡: {grpo_config.learning_rate}\")\n",
    "    print(f\"  - æœ€å¤§æ­¥æ•°: {grpo_config.max_steps}\")\n",
    "    print(f\"  - æ¯è½®ç”Ÿæˆæ•°: {grpo_config.num_generations}\")\n",
    "else:\n",
    "    print(f\"  - è®­ç»ƒæ–¹æ³•: ç›‘ç£å­¦ä¹  (SFT)\")\n",
    "    print(f\"  - å­¦ä¹ ç‡: {training_args.learning_rate}\")\n",
    "    print(f\"  - è®­ç»ƒè½®æ•°: {training_args.num_train_epochs}\")\n",
    "print(f\"  - æ‰¹æ¬¡å¤§å°: 1\")\n",
    "print(f\"  - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4540cf",
   "metadata": {},
   "source": [
    "## 8. å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "277cea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹GRPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒ...\n",
      "é¢„è®¡è®­ç»ƒæ—¶é—´: 100 åˆ†é’Ÿ\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 47 | Num Epochs = 5 | Total steps = 50\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 17,432,576 of 1,738,007,552 (1.00% trained)\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'max_length': 32768, 'bos_token_id': 151643}. If this is not desired, please set these values explicitly.\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'max_length': 32768, 'bos_token_id': 151643}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 21:33, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / portfolio_reward_function / mean</th>\n",
       "      <th>rewards / portfolio_reward_function / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>241.225000</td>\n",
       "      <td>115.600000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>129.626668</td>\n",
       "      <td>64.400000</td>\n",
       "      <td>173.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>240.875000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>103.550000</td>\n",
       "      <td>44.800000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>251.025000</td>\n",
       "      <td>180.800000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>49.200000</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>71.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>241.512500</td>\n",
       "      <td>103.800000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>82.400000</td>\n",
       "      <td>52.600000</td>\n",
       "      <td>112.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>240.762500</td>\n",
       "      <td>96.200000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>92.033334</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>140.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>248.150000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>88.100000</td>\n",
       "      <td>81.800000</td>\n",
       "      <td>94.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>241.112500</td>\n",
       "      <td>94.400000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>137.800000</td>\n",
       "      <td>94.400000</td>\n",
       "      <td>170.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>245.575000</td>\n",
       "      <td>92.400000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>41.200000</td>\n",
       "      <td>76.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>239.425000</td>\n",
       "      <td>89.800000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>135.733334</td>\n",
       "      <td>89.800000</td>\n",
       "      <td>180.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>242.237500</td>\n",
       "      <td>95.800000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>52.200000</td>\n",
       "      <td>44.600000</td>\n",
       "      <td>59.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âœ… GRPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒå®Œæˆï¼\n",
      "ğŸ’¾ ä¿å­˜æ¨¡å‹...\n",
      "âœ… æ¨¡å‹å·²ä¿å­˜åˆ° 'portfolio_model' ç›®å½•\n",
      "\n",
      "ğŸ“Š è®­ç»ƒç»Ÿè®¡:\n",
      "  - æœ€ç»ˆè®­ç»ƒæŸå¤±: 0.0000\n",
      "  - è®­ç»ƒæ–¹æ³•: GRPOå¼ºåŒ–å­¦ä¹ \n",
      "  - è®­ç»ƒæ ·æœ¬æ•°: 47\n",
      "âœ… æ¨¡å‹å·²ä¿å­˜åˆ° 'portfolio_model' ç›®å½•\n",
      "\n",
      "ğŸ“Š è®­ç»ƒç»Ÿè®¡:\n",
      "  - æœ€ç»ˆè®­ç»ƒæŸå¤±: 0.0000\n",
      "  - è®­ç»ƒæ–¹æ³•: GRPOå¼ºåŒ–å­¦ä¹ \n",
      "  - è®­ç»ƒæ ·æœ¬æ•°: 47\n"
     ]
    }
   ],
   "source": [
    "# å¼€å§‹è®­ç»ƒ\n",
    "training_method = \"GRPOå¼ºåŒ–å­¦ä¹ \" if GRPO_AVAILABLE else \"ç›‘ç£å­¦ä¹ \"\n",
    "print(f\"ğŸš€ å¼€å§‹{training_method}è®­ç»ƒ...\")\n",
    "\n",
    "if GRPO_AVAILABLE:\n",
    "    estimated_time = grpo_config.max_steps * 2\n",
    "else:\n",
    "    estimated_time = int(training_args.num_train_epochs * len(supervised_train) / 4)\n",
    "\n",
    "print(f\"é¢„è®¡è®­ç»ƒæ—¶é—´: {estimated_time} åˆ†é’Ÿ\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "try:\n",
    "    # å¯åŠ¨è®­ç»ƒ\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"âœ… {training_method}è®­ç»ƒå®Œæˆï¼\")\n",
    "    \n",
    "    # ä¿å­˜æ¨¡å‹\n",
    "    print(\"ğŸ’¾ ä¿å­˜æ¨¡å‹...\")\n",
    "    if USE_UNSLOTH:\n",
    "        model.save_pretrained(\"portfolio_model\")\n",
    "        tokenizer.save_pretrained(\"portfolio_model\")\n",
    "    else:\n",
    "        trainer.save_model(\"portfolio_model\")\n",
    "    \n",
    "    print(\"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° 'portfolio_model' ç›®å½•\")\n",
    "    \n",
    "    # æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡\n",
    "    print(\"\\nğŸ“Š è®­ç»ƒç»Ÿè®¡:\")\n",
    "    if hasattr(trainer.state, 'log_history') and trainer.state.log_history:\n",
    "        last_log = trainer.state.log_history[-1]\n",
    "        if 'train_loss' in last_log:\n",
    "            print(f\"  - æœ€ç»ˆè®­ç»ƒæŸå¤±: {last_log['train_loss']:.4f}\")\n",
    "        if 'eval_loss' in last_log:\n",
    "            print(f\"  - æœ€ç»ˆéªŒè¯æŸå¤±: {last_log['eval_loss']:.4f}\")\n",
    "    \n",
    "    print(f\"  - è®­ç»ƒæ–¹æ³•: {training_method}\")\n",
    "    print(f\"  - è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset) if GRPO_AVAILABLE else len(supervised_train)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}\")\n",
    "    print(f\"é”™è¯¯ç±»å‹: {type(e).__name__}\")\n",
    "    \n",
    "    # æä¾›è°ƒè¯•ä¿¡æ¯\n",
    "    if \"CUDA\" in str(e):\n",
    "        print(\"ğŸ’¡ å»ºè®®: GPUå†…å­˜ä¸è¶³ï¼Œå°è¯•å‡å°‘batch_sizeæˆ–ä½¿ç”¨CPU\")\n",
    "    elif \"module\" in str(e).lower():\n",
    "        print(\"ğŸ’¡ å»ºè®®: ä¾èµ–åº“ç‰ˆæœ¬é—®é¢˜ï¼Œè¯·æ£€æŸ¥å®‰è£…\")\n",
    "    else:\n",
    "        print(\"ğŸ’¡ å»ºè®®: æ£€æŸ¥æ•°æ®æ ¼å¼å’Œæ¨¡å‹é…ç½®\")\n",
    "    \n",
    "    # å°è¯•ä¿å­˜å½“å‰çŠ¶æ€\n",
    "    try:\n",
    "        print(\"ğŸ”„ å°è¯•ä¿å­˜å½“å‰æ¨¡å‹çŠ¶æ€...\")\n",
    "        if USE_UNSLOTH:\n",
    "            model.save_pretrained(\"portfolio_model_checkpoint\")\n",
    "        else:\n",
    "            trainer.save_model(\"portfolio_model_checkpoint\")\n",
    "        print(\"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜\")\n",
    "    except:\n",
    "        print(\"âŒ æ— æ³•ä¿å­˜æ£€æŸ¥ç‚¹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745cbf25",
   "metadata": {},
   "source": [
    "## 9. æ¨¡å‹æµ‹è¯•å’Œè¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b0844c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª æµ‹è¯•æ¨¡å‹...\n",
      "ğŸ“Š ç”ŸæˆæŠ•èµ„å»ºè®®...\n",
      "\n",
      "ğŸ¯ æ¨¡å‹å›ç­”:\n",
      "==================================================\n",
      "<reasoning>\n",
      "1. ä»æŠ€æœ¯æŒ‡æ ‡æ¥çœ‹ï¼ŒAAPLå’ŒAMZNéƒ½ç›¸å¯¹20æ—¥å‡çº¿æœ‰è½»å¾®çš„æ­£å‘ç§»åŠ¨ï¼Œè¡¨æ˜å®ƒä»¬å¯èƒ½ä»æœ‰ä¸Šæ¶¨ç©ºé—´ã€‚\n",
      "2. GOOGLå’ŒMETAçš„ç›¸å¯¹20æ—¥å‡çº¿ä¹Ÿæœ‰æ‰€æå‡ï¼Œæ˜¾ç¤ºå‡ºè¿™äº›è‚¡ç¥¨è¿‘æœŸèµ°åŠ¿è¾ƒä¸ºå¥åº·ã€‚\n",
      "3. NVDAåˆ™æ˜¯ä¸€ä¸ªå€¼å¾—å…³æ³¨çš„è‚¡ç¥¨ï¼Œå…¶ç›¸å¯¹äº20æ—¥çš„è´Ÿå‘ç§»åŠ¨æ˜¾ç¤ºå…¶ä»·æ ¼ä¸‹æŒ«ã€‚\n",
      "4. MSFTçš„ç›¸å¯¹20æ—¥å‡çº¿å°å¹…ä¸Šå‡ï¼Œæ˜¾ç¤ºå…¶è¿‘æœŸæ²¡æœ‰æ˜æ˜¾çš„ä¸‹è·Œè¶‹åŠ¿ã€‚\n",
      "5. TSLAçš„ç›¸å¯¹20æ—¥å‡çº¿æ˜æ˜¾ä¸‹é™ï¼Œè¡¨æ˜å…¶è¿‘æœŸæœ‰æ‰€ä¸‹è·Œï¼Œéœ€è¦è­¦æƒ•ã€‚\n",
      "6. ç»¼åˆåˆ†æï¼Œå»ºè®®æŠ•èµ„ç»„åˆä¸­å¯è€ƒè™‘å¢åŠ å¯¹AAPLå’ŒAMZNçš„æŒè‚¡ï¼ŒåŒæ—¶ä¿æŒå¯¹GOOGLå’ŒMETAçš„æŒè‚¡ï¼Œé€‚åº¦å‡å°‘å¯¹NVDAå’ŒTSLAçš„æŒè‚¡ã€‚\n",
      "</reasoning>\n",
      "<answer>\n",
      "{\n",
      "  \"AAPL\": 0.2,\n",
      "  \"AMZN\": 0.2,\n",
      "  \"GOOGL\": 0.2,\n",
      "  \"META\": 0.2,\n",
      "  \"MSFT\": 0.1,\n",
      "  \"NVDA\": 0.1,\n",
      "  \"TSLA\": 0.1\n",
      "}\n",
      "</answer>\n",
      "==================================================\n",
      "\n",
      "ğŸ“ˆ å›ç­”åˆ†æ:\n",
      "âœ… åŒ…å«æ¨ç†è¿‡ç¨‹\n",
      "âœ… åŒ…å«æ˜ç¡®ç­”æ¡ˆ\n",
      "âœ… æˆåŠŸè§£ææƒé‡: {'AAPL': 0.2, 'AMZN': 0.2, 'GOOGL': 0.2, 'META': 0.2, 'MSFT': 0.1, 'NVDA': 0.1, 'TSLA': 0.1}\n",
      "æƒé‡æ€»å’Œ: 1.100\n",
      "\n",
      "ğŸ¯ æ¨¡å‹å›ç­”:\n",
      "==================================================\n",
      "<reasoning>\n",
      "1. ä»æŠ€æœ¯æŒ‡æ ‡æ¥çœ‹ï¼ŒAAPLå’ŒAMZNéƒ½ç›¸å¯¹20æ—¥å‡çº¿æœ‰è½»å¾®çš„æ­£å‘ç§»åŠ¨ï¼Œè¡¨æ˜å®ƒä»¬å¯èƒ½ä»æœ‰ä¸Šæ¶¨ç©ºé—´ã€‚\n",
      "2. GOOGLå’ŒMETAçš„ç›¸å¯¹20æ—¥å‡çº¿ä¹Ÿæœ‰æ‰€æå‡ï¼Œæ˜¾ç¤ºå‡ºè¿™äº›è‚¡ç¥¨è¿‘æœŸèµ°åŠ¿è¾ƒä¸ºå¥åº·ã€‚\n",
      "3. NVDAåˆ™æ˜¯ä¸€ä¸ªå€¼å¾—å…³æ³¨çš„è‚¡ç¥¨ï¼Œå…¶ç›¸å¯¹äº20æ—¥çš„è´Ÿå‘ç§»åŠ¨æ˜¾ç¤ºå…¶ä»·æ ¼ä¸‹æŒ«ã€‚\n",
      "4. MSFTçš„ç›¸å¯¹20æ—¥å‡çº¿å°å¹…ä¸Šå‡ï¼Œæ˜¾ç¤ºå…¶è¿‘æœŸæ²¡æœ‰æ˜æ˜¾çš„ä¸‹è·Œè¶‹åŠ¿ã€‚\n",
      "5. TSLAçš„ç›¸å¯¹20æ—¥å‡çº¿æ˜æ˜¾ä¸‹é™ï¼Œè¡¨æ˜å…¶è¿‘æœŸæœ‰æ‰€ä¸‹è·Œï¼Œéœ€è¦è­¦æƒ•ã€‚\n",
      "6. ç»¼åˆåˆ†æï¼Œå»ºè®®æŠ•èµ„ç»„åˆä¸­å¯è€ƒè™‘å¢åŠ å¯¹AAPLå’ŒAMZNçš„æŒè‚¡ï¼ŒåŒæ—¶ä¿æŒå¯¹GOOGLå’ŒMETAçš„æŒè‚¡ï¼Œé€‚åº¦å‡å°‘å¯¹NVDAå’ŒTSLAçš„æŒè‚¡ã€‚\n",
      "</reasoning>\n",
      "<answer>\n",
      "{\n",
      "  \"AAPL\": 0.2,\n",
      "  \"AMZN\": 0.2,\n",
      "  \"GOOGL\": 0.2,\n",
      "  \"META\": 0.2,\n",
      "  \"MSFT\": 0.1,\n",
      "  \"NVDA\": 0.1,\n",
      "  \"TSLA\": 0.1\n",
      "}\n",
      "</answer>\n",
      "==================================================\n",
      "\n",
      "ğŸ“ˆ å›ç­”åˆ†æ:\n",
      "âœ… åŒ…å«æ¨ç†è¿‡ç¨‹\n",
      "âœ… åŒ…å«æ˜ç¡®ç­”æ¡ˆ\n",
      "âœ… æˆåŠŸè§£ææƒé‡: {'AAPL': 0.2, 'AMZN': 0.2, 'GOOGL': 0.2, 'META': 0.2, 'MSFT': 0.1, 'NVDA': 0.1, 'TSLA': 0.1}\n",
      "æƒé‡æ€»å’Œ: 1.100\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹\n",
    "def test_portfolio_model(model, tokenizer, test_prompt, max_length=512):\n",
    "    \"\"\"æµ‹è¯•æŠ•èµ„ç»„åˆæ¨¡å‹\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # å‡†å¤‡è¾“å…¥\n",
    "    inputs = tokenizer(\n",
    "        test_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1500\n",
    "    ).to(model.device if hasattr(model, 'device') else 'cpu')\n",
    "    \n",
    "    # ç”Ÿæˆå›ç­”\n",
    "    with torch.no_grad():\n",
    "        if USE_UNSLOTH:\n",
    "            # Unslothæ¨¡å‹ç”Ÿæˆ\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        else:\n",
    "            # æ ‡å‡†transformersç”Ÿæˆ\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_new_tokens=max_length,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1,\n",
    "            )\n",
    "    \n",
    "    # è§£ç ç»“æœ\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # æå–æ¨¡å‹å›ç­”éƒ¨åˆ†ï¼ˆå»é™¤è¾“å…¥promptï¼‰\n",
    "    response = generated_text[len(tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)):]\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "# åˆ›å»ºæµ‹è¯•ç”¨ä¾‹\n",
    "print(\"ğŸ§ª æµ‹è¯•æ¨¡å‹...\")\n",
    "\n",
    "test_scenario = {\n",
    "    \"date\": \"2025-09-26\",\n",
    "    \"market_data\": {\n",
    "        \"AAPL\": {\"price\": 225.50, \"daily_return\": 1.2, \"vs_sma20\": 3.5},\n",
    "        \"AMZN\": {\"price\": 185.30, \"daily_return\": -0.8, \"vs_sma20\": -2.1},\n",
    "        \"GOOGL\": {\"price\": 165.80, \"daily_return\": 0.5, \"vs_sma20\": 1.8},\n",
    "        \"META\": {\"price\": 520.40, \"daily_return\": 2.1, \"vs_sma20\": 5.2},\n",
    "        \"MSFT\": {\"price\": 415.70, \"daily_return\": 0.3, \"vs_sma20\": 0.9},\n",
    "        \"NVDA\": {\"price\": 125.90, \"daily_return\": -1.5, \"vs_sma20\": -4.3},\n",
    "        \"TSLA\": {\"price\": 245.60, \"daily_return\": -2.3, \"vs_sma20\": -6.8}\n",
    "    }\n",
    "}\n",
    "\n",
    "# æ„å»ºæµ‹è¯•prompt\n",
    "test_question = f\"\"\"\n",
    "æ—¥æœŸ: {test_scenario['date']}\n",
    "å¸‚åœºæ•°æ®:\n",
    "\"\"\"\n",
    "\n",
    "for ticker, data in test_scenario['market_data'].items():\n",
    "    test_question += f\"{ticker}: ä»·æ ¼${data['price']}, æ—¥æ¶¨è·Œ{data['daily_return']:+.1f}%, ç›¸å¯¹20æ—¥å‡çº¿{data['vs_sma20']:+.1f}%\\n\"\n",
    "\n",
    "test_question += \"\\nè¯·åˆ†æå½“å‰å¸‚åœºæƒ…å†µå¹¶ç»™å‡ºMAG7è‚¡ç¥¨çš„æŠ•èµ„ç»„åˆæƒé‡å»ºè®®ã€‚\"\n",
    "\n",
    "test_prompt = f\"\"\"\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "ç”¨æˆ·: {test_question.strip()}\n",
    "\n",
    "åŠ©æ‰‹: \n",
    "\"\"\"\n",
    "\n",
    "# è¿›è¡ŒåŸºç¡€æµ‹è¯•ï¼ˆä¸ä¾èµ–è®­ç»ƒç»“æœï¼‰\n",
    "print(\"ğŸ“Š ç”ŸæˆæŠ•èµ„å»ºè®®...\")\n",
    "try:\n",
    "    response = test_portfolio_model(model, tokenizer, test_prompt)\n",
    "    \n",
    "    print(\"\\nğŸ¯ æ¨¡å‹å›ç­”:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(response)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # åˆ†æå›ç­”è´¨é‡\n",
    "    print(\"\\nğŸ“ˆ å›ç­”åˆ†æ:\")\n",
    "    if \"<reasoning>\" in response and \"</reasoning>\" in response:\n",
    "        print(\"âœ… åŒ…å«æ¨ç†è¿‡ç¨‹\")\n",
    "    else:\n",
    "        print(\"âŒ ç¼ºå°‘æ¨ç†è¿‡ç¨‹\")\n",
    "    \n",
    "    if \"<answer>\" in response and \"</answer>\" in response:\n",
    "        print(\"âœ… åŒ…å«æ˜ç¡®ç­”æ¡ˆ\")\n",
    "        answer_text = extract_xml_answer(response)\n",
    "        try:\n",
    "            weights = parse_portfolio_weights(answer_text)\n",
    "            print(f\"âœ… æˆåŠŸè§£ææƒé‡: {weights}\")\n",
    "            total_weight = sum(weights.values())\n",
    "            print(f\"æƒé‡æ€»å’Œ: {total_weight:.3f}\")\n",
    "        except:\n",
    "            print(\"âŒ æƒé‡è§£æå¤±è´¥\")\n",
    "    else:\n",
    "        print(\"âŒ ç¼ºå°‘æ˜ç¡®ç­”æ¡ˆ\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}\")\n",
    "    print(\"è¿™å¯èƒ½æ˜¯ç”±äºæ¨¡å‹æœªå®Œæˆè®­ç»ƒæˆ–é…ç½®é—®é¢˜å¯¼è‡´çš„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039fe5d",
   "metadata": {},
   "source": [
    "## 10. ä¿å­˜å’Œå¯¼å‡ºæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f08af7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ ä¿å­˜æ¨¡å‹æƒé‡å’Œé…ç½®...\n",
      "ğŸ’¾ ä¿å­˜LoRAæƒé‡...\n",
      "âŒ ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: 'Qwen3ForCausalLM' object has no attribute 'save_lora'\n",
      "å°è¯•åŸºæœ¬ä¿å­˜...\n",
      "âœ… æ¨¡å‹æƒé‡å·²ä¿å­˜ä¸º portfolio_model_weights.pth\n",
      "\n",
      "ğŸ“– ä½¿ç”¨æŒ‡å—å·²åˆ›å»º: MODEL_USAGE_GUIDE.md\n",
      "âœ… æ¨¡å‹æƒé‡å·²ä¿å­˜ä¸º portfolio_model_weights.pth\n",
      "\n",
      "ğŸ“– ä½¿ç”¨æŒ‡å—å·²åˆ›å»º: MODEL_USAGE_GUIDE.md\n"
     ]
    }
   ],
   "source": [
    "# ä¿å­˜å’Œå¯¼å‡ºæ¨¡å‹\n",
    "print(\"ğŸ’¾ ä¿å­˜æ¨¡å‹æƒé‡å’Œé…ç½®...\")\n",
    "\n",
    "try:\n",
    "    if USE_UNSLOTH:\n",
    "        # ä¿å­˜LoRAæƒé‡\n",
    "        print(\"ğŸ’¾ ä¿å­˜LoRAæƒé‡...\")\n",
    "        model.save_lora(\"portfolio_lora\")\n",
    "        \n",
    "        # ä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆå¯é€‰ï¼‰\n",
    "        print(\"ğŸ’¾ ä¿å­˜å®Œæ•´æ¨¡å‹...\")\n",
    "        model.save_pretrained_merged(\n",
    "            \"portfolio_complete\", \n",
    "            tokenizer, \n",
    "            save_method=\"merged_16bit\"\n",
    "        )\n",
    "        \n",
    "        saved_files = [\"portfolio_lora/\", \"portfolio_complete/\"]\n",
    "        \n",
    "    else:\n",
    "        # ä½¿ç”¨æ ‡å‡†æ–¹æ³•ä¿å­˜\n",
    "        print(\"ğŸ’¾ ä¿å­˜PEFTæ¨¡å‹...\")\n",
    "        model.save_pretrained(\"portfolio_peft\")\n",
    "        tokenizer.save_pretrained(\"portfolio_peft\")\n",
    "        \n",
    "        # åˆå¹¶å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹\n",
    "        print(\"ğŸ’¾ åˆå¹¶å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹...\")\n",
    "        from peft import PeftModel\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"google/gemma-2-2b-it\",\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"cpu\"  # åœ¨CPUä¸Šåˆå¹¶ä»¥èŠ‚çœGPUå†…å­˜\n",
    "        )\n",
    "        merged_model = PeftModel.from_pretrained(base_model, \"portfolio_peft\")\n",
    "        merged_model = merged_model.merge_and_unload()\n",
    "        \n",
    "        merged_model.save_pretrained(\"portfolio_complete\")\n",
    "        tokenizer.save_pretrained(\"portfolio_complete\")\n",
    "        \n",
    "        saved_files = [\"portfolio_peft/\", \"portfolio_complete/\"]\n",
    "    \n",
    "    # ä¿å­˜è®­ç»ƒé…ç½®å’Œå…ƒæ•°æ®\n",
    "    training_metadata = {\n",
    "        \"model_name\": \"portfolio_optimization_model\",\n",
    "        \"base_model\": \"google/gemma-2-2b-it\" if not USE_UNSLOTH else \"unsloth/gemma-2-2b-it-bnb-4bit\",\n",
    "        \"training_method\": \"GRPO\" if GRPO_AVAILABLE else \"Supervised Fine-tuning\",\n",
    "        \"framework\": \"Unsloth\" if USE_UNSLOTH else \"Transformers + PEFT\",\n",
    "        \"dataset\": \"MAG7_portfolio_optimization\",\n",
    "        \"training_samples\": len(training_samples),\n",
    "        \"tickers\": tickers,\n",
    "        \"reward_components\": {\n",
    "            \"portfolio_return\": 0.4,\n",
    "            \"weight_validity\": 0.2,\n",
    "            \"reasoning_quality\": 0.25,\n",
    "            \"format_correctness\": 0.15\n",
    "        } if GRPO_AVAILABLE else None,\n",
    "        \"training_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"model_size\": f\"{model.num_parameters():,} parameters\"\n",
    "    }\n",
    "    \n",
    "    with open(\"portfolio_model_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(training_metadata, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"âœ… æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜ï¼\")\n",
    "    print(\"\\nğŸ“ ä¿å­˜çš„æ–‡ä»¶:\")\n",
    "    for file_path in saved_files:\n",
    "        print(f\"  - {file_path}\")\n",
    "    print(\"  - portfolio_model_metadata.json (è®­ç»ƒå…ƒæ•°æ®)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}\")\n",
    "    print(\"å°è¯•åŸºæœ¬ä¿å­˜...\")\n",
    "    \n",
    "    try:\n",
    "        # åŸºæœ¬ä¿å­˜æ–¹æ³•\n",
    "        torch.save(model.state_dict(), \"portfolio_model_weights.pth\")\n",
    "        print(\"âœ… æ¨¡å‹æƒé‡å·²ä¿å­˜ä¸º portfolio_model_weights.pth\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ åŸºæœ¬ä¿å­˜ä¹Ÿå¤±è´¥: {e2}\")\n",
    "\n",
    "# åˆ›å»ºä½¿ç”¨è¯´æ˜\n",
    "usage_guide = f\"\"\"\n",
    "# æŠ•èµ„ç»„åˆä¼˜åŒ–æ¨¡å‹ä½¿ç”¨æŒ‡å—\n",
    "\n",
    "## æ¨¡å‹ä¿¡æ¯\n",
    "- è®­ç»ƒæ¡†æ¶: {\"Unsloth\" if USE_UNSLOTH else \"Transformers + PEFT\"}\n",
    "- è®­ç»ƒæ–¹æ³•: {\"GRPOå¼ºåŒ–å­¦ä¹ \" if GRPO_AVAILABLE else \"ç›‘ç£å­¦ä¹ \"}\n",
    "- åŸºç¡€æ¨¡å‹: {\"unsloth/gemma-2-2b-it-bnb-4bit\" if USE_UNSLOTH else \"google/gemma-2-2b-it\"}\n",
    "- å‚æ•°é‡: {model.num_parameters():,}\n",
    "\n",
    "## æ¨¡å‹åŠ è½½ç¤ºä¾‹\n",
    "\n",
    "```python\n",
    "{\"# ä½¿ç”¨UnslothåŠ è½½\" if USE_UNSLOTH else \"# ä½¿ç”¨æ ‡å‡†transformersåŠ è½½\"}\n",
    "{\"from unsloth import FastLanguageModel\" if USE_UNSLOTH else \"from transformers import AutoModelForCausalLM, AutoTokenizer\"}\n",
    "\n",
    "{\"model, tokenizer = FastLanguageModel.from_pretrained('./portfolio_complete')\" if USE_UNSLOTH else '''\n",
    "model = AutoModelForCausalLM.from_pretrained('./portfolio_complete')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./portfolio_complete')\n",
    "'''}\n",
    "```\n",
    "\n",
    "## ä½¿ç”¨ç¤ºä¾‹\n",
    "\n",
    "```python\n",
    "# æ„å»ºæŠ•èµ„æŸ¥è¯¢\n",
    "prompt = '''\n",
    "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æŠ•èµ„ç»„åˆç®¡ç†ä¸“å®¶ã€‚è¯·æ ¹æ®æä¾›çš„å¸‚åœºæ•°æ®å’ŒæŠ€æœ¯æŒ‡æ ‡ï¼Œä¸ºæŠ•èµ„è€…æä¾›æŠ•èµ„å»ºè®®ã€‚\n",
    "\n",
    "æ—¥æœŸ: 2025-09-26\n",
    "å¸‚åœºæ•°æ®:\n",
    "AAPL: ä»·æ ¼$225.50, æ—¥æ¶¨è·Œ+1.2%, ç›¸å¯¹20æ—¥å‡çº¿+3.5%\n",
    "TSLA: ä»·æ ¼$245.60, æ—¥æ¶¨è·Œ-2.3%, ç›¸å¯¹20æ—¥å‡çº¿-6.8%\n",
    "...\n",
    "\n",
    "è¯·åˆ†æå½“å‰å¸‚åœºæƒ…å†µå¹¶ç»™å‡ºMAG7è‚¡ç¥¨çš„æŠ•èµ„ç»„åˆæƒé‡å»ºè®®ã€‚\n",
    "\n",
    "è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š\n",
    "<reasoning>\n",
    "è¯¦ç»†åˆ†æå¸‚åœºæƒ…å†µã€æŠ€æœ¯æŒ‡æ ‡å’ŒæŠ•èµ„é€»è¾‘...\n",
    "</reasoning>\n",
    "<answer>\n",
    "å…·ä½“çš„æŠ•èµ„ç»„åˆæƒé‡å»ºè®®ï¼ˆJSONæ ¼å¼ï¼‰\n",
    "</answer>\n",
    "'''\n",
    "\n",
    "# ç”Ÿæˆå»ºè®®\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=512, \n",
    "    temperature=0.7, \n",
    "    do_sample=True\n",
    ")\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "## æ³¨æ„äº‹é¡¹\n",
    "âš ï¸ æœ¬æ¨¡å‹ä»…ç”¨äºæ•™è‚²å’Œç ”ç©¶ç›®çš„ï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚\n",
    "âš ï¸ å®é™…æŠ•èµ„å†³ç­–åº”å’¨è¯¢ä¸“ä¸šé‡‘èé¡¾é—®ã€‚\n",
    "âš ï¸ å¸‚åœºæœ‰é£é™©ï¼ŒæŠ•èµ„éœ€è°¨æ…ã€‚\n",
    "\"\"\"\n",
    "\n",
    "with open(\"MODEL_USAGE_GUIDE.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(usage_guide)\n",
    "\n",
    "print(\"\\nğŸ“– ä½¿ç”¨æŒ‡å—å·²åˆ›å»º: MODEL_USAGE_GUIDE.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799af38",
   "metadata": {},
   "source": [
    "## 11. æ€»ç»“å’Œåç»­æ”¹è¿›æ–¹å‘\n",
    "\n",
    "### ğŸ¯ é¡¹ç›®æˆæœ\n",
    "\n",
    "1. **å¼ºåŒ–å­¦ä¹ æ¨¡å‹**: æˆåŠŸä½¿ç”¨GRPOæ–¹æ³•è®­ç»ƒGemma 1Bæ¨¡å‹è¿›è¡ŒæŠ•èµ„ç»„åˆä¼˜åŒ–\n",
    "2. **å¤šç»´åº¦å¥–åŠ±å‡½æ•°**: ç»¼åˆè€ƒè™‘æŠ•èµ„æ”¶ç›Šã€æƒé‡åˆç†æ€§ã€æ¨ç†è´¨é‡å’Œæ ¼å¼æ­£ç¡®æ€§\n",
    "3. **çœŸå®æ•°æ®è®­ç»ƒ**: åŸºäºMAG7è‚¡ç¥¨çš„å®é™…å¸‚åœºæ•°æ®ç”Ÿæˆè®­ç»ƒæ ·æœ¬\n",
    "4. **ç»“æ„åŒ–è¾“å‡º**: æ¨¡å‹èƒ½å¤Ÿç”ŸæˆåŒ…å«æ¨ç†è¿‡ç¨‹å’Œå…·ä½“æƒé‡çš„æŠ•èµ„å»ºè®®\n",
    "\n",
    "### ğŸ“ˆ æ ¸å¿ƒç‰¹æ€§\n",
    "\n",
    "- **æ•°æ®é©±åŠ¨**: åŸºäº20å¹´å†å²æ•°æ®çš„æŠ€æœ¯æŒ‡æ ‡åˆ†æ\n",
    "- **é£é™©æ„è¯†**: å¥–åŠ±å‡½æ•°é¼“åŠ±æƒé‡åˆ†æ•£å’Œé£é™©æ§åˆ¶\n",
    "- **å¯è§£é‡Šæ€§**: è¦æ±‚æ¨¡å‹æä¾›è¯¦ç»†çš„æŠ•èµ„æ¨ç†è¿‡ç¨‹\n",
    "- **å®æ—¶é€‚åº”**: èƒ½å¤Ÿæ ¹æ®å½“å‰å¸‚åœºçŠ¶å†µè°ƒæ•´æŠ•èµ„ç­–ç•¥\n",
    "\n",
    "### ğŸ”„ æ”¹è¿›æ–¹å‘\n",
    "\n",
    "1. **æ‰©å¤§æ•°æ®é›†**: å¢åŠ æ›´å¤šæ ·æœ¬å’Œæ—¶é—´æ®µ\n",
    "2. **ä¼˜åŒ–å¥–åŠ±å‡½æ•°**: åŠ å…¥æ›´å¤šé‡‘èæŒ‡æ ‡ï¼ˆå¦‚å¤æ™®æ¯”ç‡ã€æœ€å¤§å›æ’¤ï¼‰\n",
    "3. **æ¨¡å‹é›†æˆ**: ç»“åˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ\n",
    "4. **å®æ—¶æ›´æ–°**: é›†æˆå®æ—¶å¸‚åœºæ•°æ®API\n",
    "5. **é£é™©ç®¡ç†**: æ·»åŠ æ›´ä¸¥æ ¼çš„é£é™©æ§åˆ¶æœºåˆ¶\n",
    "\n",
    "### âš ï¸ ä½¿ç”¨å£°æ˜\n",
    "\n",
    "**æœ¬æ¨¡å‹ä»…ç”¨äºæ•™è‚²å’Œç ”ç©¶ç›®çš„ï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚å®é™…æŠ•èµ„å†³ç­–åº”å’¨è¯¢ä¸“ä¸šé‡‘èé¡¾é—®ã€‚**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xiaojiucai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
