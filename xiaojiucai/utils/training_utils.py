"""
è®­ç»ƒå·¥å…·æ¨¡å—

åŒ…å«æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€ä¿å­˜ç­‰å·¥å…·å‡½æ•°
"""

import torch
import torch.nn as nn
import torch.optim as optim
import os
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import numpy as np


class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""
    
    def __init__(self):
        # æ•°æ®å¤„ç†å‚æ•°
        self.window_size = 200
        self.batch_size = 128
        
        # è®­ç»ƒè¶…å‚æ•°
        self.finetune_timesfm = True
        self.risk_aversion = 1.0
        self.sharpe_weight = 0.5
        
        # è®­ç»ƒæ§åˆ¶å‚æ•°
        self.num_epochs = 20
        self.patience = 20
        self.grad_clip_value = 1.0
        
        # å­¦ä¹ ç‡å‚æ•°
        self.timesfm_lr = 1e-5
        self.portfolio_lr = 1e-3
        
        # æ¨¡å‹ä¿å­˜å‚æ•°
        self.checkpoint_interval = 5
        self.max_checkpoints = 5
        self.max_best_models = 3
    
    def update(self, **kwargs):
        """æ›´æ–°é…ç½®å‚æ•°"""
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
            else:
                raise ValueError(f"Unknown parameter: {key}")
    
    def print_config(self):
        """æ‰“å°é…ç½®ä¿¡æ¯"""
        print("ğŸ¯ è®­ç»ƒé…ç½®æ€»è§ˆ:")
        print(f"  æ•°æ®å‚æ•°: window_size={self.window_size}, batch_size={self.batch_size}")
        print(f"  è®­ç»ƒå‚æ•°: epochs={self.num_epochs}, patience={self.patience}")
        print(f"  ä¼˜åŒ–å‚æ•°: TimesFM_lr={self.timesfm_lr}, portfolio_lr={self.portfolio_lr}")
        print(f"  æŸå¤±å‚æ•°: risk_aversion={self.risk_aversion}, sharpe_weight={self.sharpe_weight}")
        print(f"  æ§åˆ¶å‚æ•°: finetune_timesfm={self.finetune_timesfm}, grad_clip={self.grad_clip_value}")


def setup_optimizer_and_scheduler(model, config: TrainingConfig):
    """
    è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    
    Args:
        model: éœ€è¦è®­ç»ƒçš„æ¨¡å‹
        config: è®­ç»ƒé…ç½®
        
    Returns:
        optimizer: ä¼˜åŒ–å™¨
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
    """
    # åˆ†ç¦»å‚æ•°ç»„
    timesfm_params = []
    portfolio_params = []
    
    for name, param in model.named_parameters():
        if param.requires_grad:
            if 'timesfm' in name:
                timesfm_params.append(param)
            else:
                portfolio_params.append(param)
    
    # åˆ›å»ºåˆ†å±‚å­¦ä¹ ç‡ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam([
        {'params': timesfm_params, 'lr': config.timesfm_lr, 'name': 'timesfm'},
        {'params': portfolio_params, 'lr': config.portfolio_lr, 'name': 'portfolio'}
    ])
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )
    
    print(f"ğŸ¯ ä¼˜åŒ–å™¨é…ç½®:")
    print(f"  TimesFMå‚æ•°: {sum(p.numel() for p in timesfm_params):,} (lr={config.timesfm_lr})")
    print(f"  æŠ•èµ„ç»„åˆå±‚å‚æ•°: {sum(p.numel() for p in portfolio_params):,} (lr={config.portfolio_lr})")
    
    return optimizer, scheduler


def validate_gradient_flow(model, test_input, test_returns, loss_fn):
    """
    éªŒè¯æ¢¯åº¦ä¼ æ’­
    
    Args:
        model: æ¨¡å‹
        test_input: æµ‹è¯•è¾“å…¥
        test_returns: æµ‹è¯•å›æŠ¥ç‡
        loss_fn: æŸå¤±å‡½æ•°
        
    Returns:
        bool: æ¢¯åº¦æ˜¯å¦æ­£å¸¸ä¼ æ’­
    """
    model.train()
    
    # å‰å‘ä¼ æ’­
    weights = model(test_input)
    loss = loss_fn(weights, test_returns)
    loss.backward()
    
    # ç»Ÿè®¡æ¢¯åº¦ä¼ æ’­
    timesfm_grad_count = 0
    timesfm_total_count = 0
    
    for name, param in model.named_parameters():
        if 'timesfm' in name and param.requires_grad:
            timesfm_total_count += 1
            if param.grad is not None and param.grad.abs().sum() > 0:
                timesfm_grad_count += 1
    
    gradient_ok = timesfm_grad_count > 0
    print(f"ğŸ” æ¢¯åº¦ä¼ æ’­éªŒè¯: {'âœ“ é€šè¿‡' if gradient_ok else 'âœ— å¤±è´¥'} ({timesfm_grad_count}/{timesfm_total_count})")
    
    return gradient_ok


class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model, config: TrainingConfig, loss_fn, device='cpu', model_save_dir='models'):
        self.model = model
        self.config = config
        self.loss_fn = loss_fn
        self.device = device
        self.model_save_dir = model_save_dir
        
        # ç¡®ä¿æ¨¡å‹ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(model_save_dir, exist_ok=True)
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer, self.scheduler = setup_optimizer_and_scheduler(model, config)
        
        # è®­ç»ƒå†å²
        self.train_losses = []
        self.val_losses = []
        self.best_val_loss = float('inf')
        self.best_model_state = None
        self.best_model_path = None  # æ·»åŠ æœ€ä½³æ¨¡å‹è·¯å¾„å±æ€§
        self.patience_counter = 0
        
        # æ¨¡å‹ä¿å­˜ç­–ç•¥
        self.saved_checkpoints = []
        self.best_models_history = []
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        epoch_loss = 0
        batch_count = 0
        
        for batch_X, batch_y in train_loader:
            batch_X = batch_X.to(self.device)
            batch_y = batch_y.to(self.device)
            
            self.optimizer.zero_grad()
            
            weights = self.model(batch_X)
            loss = self.loss_fn(weights, batch_y)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.config.finetune_timesfm:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clip_value)
            
            self.optimizer.step()
            
            epoch_loss += loss.item()
            batch_count += 1
        
        return epoch_loss / batch_count
    
    def validate_epoch(self, val_loader):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        epoch_loss = 0
        batch_count = 0
        
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X = batch_X.to(self.device)
                batch_y = batch_y.to(self.device)
                
                weights = self.model(batch_X)
                loss = self.loss_fn(weights, batch_y)
                
                epoch_loss += loss.item()
                batch_count += 1
        
        return epoch_loss / batch_count
    
    def save_checkpoint(self, epoch, train_loss, val_loss):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_path = os.path.join(
            self.model_save_dir, 
            f'timesfm_portfolio_checkpoint_epoch_{epoch+1:02d}_{self.timestamp}.pth'
        )
        
        torch.save({
            'epoch': epoch + 1,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_loss': train_loss,
            'val_loss': val_loss,
            'config': self.config.__dict__,
            'timestamp': self.timestamp
        }, checkpoint_path)
        
        self.saved_checkpoints.append(checkpoint_path)
        
        # ä¿æŒcheckpointæ•°é‡ä¸è¶…è¿‡æœ€å¤§å€¼
        if len(self.saved_checkpoints) > self.config.max_checkpoints:
            old_checkpoint = self.saved_checkpoints.pop(0)
            if os.path.exists(old_checkpoint):
                os.remove(old_checkpoint)
        
        return checkpoint_path
    
    def save_best_model(self, epoch, val_loss):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        best_model_path = os.path.join(
            self.model_save_dir,
            f'timesfm_portfolio_model_best_{self.timestamp}.pth'
        )
        
        # ä¿å­˜åˆ°æœ€ä½³æ¨¡å‹å†å²
        best_model_info = {
            'epoch': epoch + 1,
            'val_loss': val_loss,
            'model_state': self.model.state_dict().copy(),
            'optimizer_state': self.optimizer.state_dict(),
            'scheduler_state': self.scheduler.state_dict(),
            'timestamp': self.timestamp
        }
        self.best_models_history.append(best_model_info)
        
        # ä¿æŒæœ€ä½³æ¨¡å‹å†å²æ•°é‡ä¸è¶…è¿‡æœ€å¤§å€¼
        if len(self.best_models_history) > self.config.max_best_models:
            self.best_models_history.pop(0)
        
        self.best_val_loss = val_loss
        self.best_model_state = self.model.state_dict().copy()
        self.best_model_path = best_model_path  # è®¾ç½®æœ€ä½³æ¨¡å‹è·¯å¾„
        
        # ä¿å­˜å…¨å±€æœ€ä½³æ¨¡å‹
        torch.save({
            'epoch': epoch + 1,
            'model_state_dict': self.best_model_state,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': self.best_val_loss,
            'config': self.config.__dict__,
            'timestamp': self.timestamp
        }, best_model_path)
        
        return best_model_path
    
    def train(self, train_loader, val_loader):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ (è®¾å¤‡: {self.device})")
        print(f"  è®­ç»ƒé›†: {len(train_loader.dataset)} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(val_loader.dataset)} æ ·æœ¬")
        print("=" * 80)
        
        best_model_path = None
        
        for epoch in range(self.config.num_epochs):
            # è®­ç»ƒå’ŒéªŒè¯
            train_loss = self.train_epoch(train_loader)
            val_loss = self.validate_epoch(val_loader)
            
            # è®°å½•æŸå¤±
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            
            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
            self.scheduler.step(val_loss)
            
            # è·å–å½“å‰å­¦ä¹ ç‡
            current_lrs = [group['lr'] for group in self.optimizer.param_groups]
            lr_info = f"LR: {current_lrs}" if len(current_lrs) > 1 else f"LR: {current_lrs[0]:.2e}"
            
            # æ¨¡å‹ä¿å­˜é€»è¾‘
            save_info = []
            
            # å®šæœŸä¿å­˜checkpoint
            if (epoch + 1) % self.config.checkpoint_interval == 0:
                checkpoint_path = self.save_checkpoint(epoch, train_loss, val_loss)
                save_info.append(f"ğŸ’¾ Checkpoint-{epoch+1}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < self.best_val_loss:
                best_model_path = self.save_best_model(epoch, val_loss)
                self.patience_counter = 0
                save_info.append(f"â­ æ–°æœ€ä½³æ¨¡å‹ ({self.best_val_loss:.4f})")
            else:
                self.patience_counter += 1
            
            # æ—©åœæ£€æŸ¥
            if self.patience_counter >= self.config.patience:
                save_info.append("â¹ï¸ æ—©åœ")
                print(f'\nEarly stopping triggered after {self.config.patience} epochs without improvement')
                break
            
            # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
            save_status = " | ".join(save_info) if save_info else "æ— ä¿å­˜"
            status = f"ç»§ç»­è®­ç»ƒ" if self.patience_counter < self.config.patience else "æ—©åœè§¦å‘"
            
            print(f'Epoch {epoch+1:2d}/{self.config.num_epochs} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | '
                  f'Best: {self.best_val_loss:.4f} | {lr_info} | {status} | {save_status}')
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œè®­ç»ƒæ€»ç»“
        final_model_path = self._save_final_model()
        self._save_training_summary(final_model_path, best_model_path)
        
        return {
            'best_model_path': best_model_path,
            'final_model_path': final_model_path,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'best_val_loss': self.best_val_loss
        }
    
    def _save_final_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        final_model_path = os.path.join(
            self.model_save_dir,
            f'timesfm_portfolio_model_final_{self.timestamp}.pth'
        )
        
        torch.save({
            'epoch': len(self.train_losses),
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'final_train_loss': self.train_losses[-1],
            'final_val_loss': self.val_losses[-1],
            'best_val_loss': self.best_val_loss,
            'config': self.config.__dict__,
            'timestamp': self.timestamp,
            'training_history': {
                'train_losses': self.train_losses,
                'val_losses': self.val_losses,
                'total_epochs': len(self.train_losses)
            }
        }, final_model_path)
        
        return final_model_path
    
    def _save_training_summary(self, final_model_path, best_model_path):
        """ä¿å­˜è®­ç»ƒæ€»ç»“"""
        training_summary = {
            'timestamp': self.timestamp,
            'configuration': self.config.__dict__,
            'training_results': {
                'final_train_loss': self.train_losses[-1],
                'final_val_loss': self.val_losses[-1], 
                'best_val_loss': self.best_val_loss,
                'total_epochs': len(self.train_losses),
                'early_stopped': self.patience_counter >= self.config.patience
            },
            'saved_models': {
                'best_model': os.path.basename(best_model_path) if best_model_path else None,
                'final_model': os.path.basename(final_model_path),
                'checkpoints': [os.path.basename(cp) for cp in self.saved_checkpoints],
                'best_models_count': len(self.best_models_history)
            }
        }
        
        summary_path = os.path.join(self.model_save_dir, f'training_summary_{self.timestamp}.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(training_summary, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“Š è®­ç»ƒæ€»ç»“å·²ä¿å­˜: {summary_path}")
        print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹: {os.path.basename(best_model_path) if best_model_path else 'æ— '}")
        print(f"ğŸ“‹ æœ€ç»ˆæ¨¡å‹: {os.path.basename(final_model_path)}")
    
    def load_checkpoint(self, checkpoint_path):
        """
        åŠ è½½æ£€æŸ¥ç‚¹å¹¶æ¢å¤è®­ç»ƒçŠ¶æ€
        
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
            
        Returns:
            checkpoint: æ£€æŸ¥ç‚¹ä¿¡æ¯å­—å…¸
        """
        print(f"ğŸ“¥ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
        
        # æ¢å¤æ¨¡å‹çŠ¶æ€
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€
        if 'optimizer_state_dict' in checkpoint:
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # æ¢å¤å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
        if 'scheduler_state_dict' in checkpoint and hasattr(self, 'scheduler'):
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        # æ¢å¤è®­ç»ƒå†å²
        if 'training_history' in checkpoint:
            history = checkpoint['training_history']
            self.train_losses = history.get('train_losses', [])
            self.val_losses = history.get('val_losses', [])
        
        # æ¢å¤æœ€ä½³éªŒè¯æŸå¤±
        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        
        # æ›´æ–°æ—¶é—´æˆ³ï¼ˆä¿æŒåŸæœ‰æ—¶é—´æˆ³ï¼‰
        if 'timestamp' in checkpoint:
            self.timestamp = checkpoint['timestamp']
        
        print(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ:")
        print(f"  - è®­ç»ƒè½®æ•°: {checkpoint.get('epoch', 'æœªçŸ¥')}")
        print(f"  - æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
        print(f"  - è®­ç»ƒå†å²é•¿åº¦: {len(self.train_losses)} è½®")
        
        return checkpoint
    
    def resume_training(self, checkpoint_path, train_loader, val_loader):
        """
        ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
        
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = self.load_checkpoint(checkpoint_path)
        
        # è®¡ç®—å·²å®Œæˆçš„è½®æ•°
        completed_epochs = len(self.train_losses)
        remaining_epochs = self.config.num_epochs - completed_epochs
        
        print(f"ğŸ”„ æ¢å¤è®­ç»ƒ:")
        print(f"  - å·²å®Œæˆè½®æ•°: {completed_epochs}")
        print(f"  - å‰©ä½™è½®æ•°: {remaining_epochs}")
        
        if remaining_epochs <= 0:
            print("âš ï¸ è®­ç»ƒå·²å®Œæˆï¼Œæ— éœ€ç»§ç»­è®­ç»ƒ")
            return {
                'best_model_path': None,
                'final_model_path': None,
                'train_losses': self.train_losses,
                'val_losses': self.val_losses,
                'best_val_loss': self.best_val_loss
            }
        
        # è°ƒæ•´è®­ç»ƒé…ç½®
        original_epochs = self.config.num_epochs
        self.config.num_epochs = remaining_epochs
        
        # ç»§ç»­è®­ç»ƒ
        print(f"â–¶ï¸ ç»§ç»­è®­ç»ƒ...")
        results = self.train(train_loader, val_loader)
        
        # æ¢å¤åŸå§‹é…ç½®
        self.config.num_epochs = original_epochs
        
        return results


def load_model_checkpoint(checkpoint_path, model_class, model_args, device='cpu'):
    """
    åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆé€šç”¨ç‰ˆæœ¬ï¼‰
    
    Args:
        checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        model_class: æ¨¡å‹ç±»
        model_args: æ¨¡å‹åˆå§‹åŒ–å‚æ•°
        device: è®¾å¤‡
        
    Returns:
        model: åŠ è½½çš„æ¨¡å‹
        checkpoint: æ£€æŸ¥ç‚¹ä¿¡æ¯
    """
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # é‡æ–°åˆ›å»ºæ¨¡å‹
    model = model_class(**model_args).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"æ¨¡å‹å·²ä» {checkpoint_path} åŠ è½½")
    print(f"æ£€æŸ¥ç‚¹ä¿¡æ¯:")
    print(f"  - è®­ç»ƒè½®æ•°: {checkpoint.get('epoch', 'æœªçŸ¥')}")
    print(f"  - éªŒè¯æŸå¤±: {checkpoint.get('val_loss', 'æœªçŸ¥')}")
    print(f"  - æ—¶é—´æˆ³: {checkpoint.get('timestamp', 'æœªçŸ¥')}")
    
    return model, checkpoint


def load_timesfm_portfolio_model(checkpoint_path, device='cpu'):
    """
    ä¸“ç”¨äºåŠ è½½TimesFMæŠ•èµ„ç»„åˆæ¨¡å‹çš„å‡½æ•°
    
    Args:
        checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡
        
    Returns:
        model: åŠ è½½çš„æ¨¡å‹
        checkpoint: æ£€æŸ¥ç‚¹ä¿¡æ¯
        config: è®­ç»ƒé…ç½®
    """
    # ä½¿ç”¨ç»å¯¹å¯¼å…¥
    from xiaojiucai.models import create_timesfm_model, TimesFMPortfolioModelV2
    
    print(f"ğŸ“¥ åŠ è½½TimesFMæŠ•èµ„ç»„åˆæ¨¡å‹: {checkpoint_path}")
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # è·å–æ¨¡å‹é…ç½®
    config_dict = checkpoint.get('config', {})
    
    # é‡æ–°åˆ›å»ºTimesFMæ¨¡å‹å’ŒåŒ…è£…å™¨
    timesfm_model, timesfm_wrapper = create_timesfm_model()
    
    # è·å–æ¨¡å‹å‚æ•°
    input_size = config_dict.get('input_size', checkpoint.get('input_size', 7))
    feature_size = config_dict.get('feature_size', checkpoint.get('feature_size', 1))
    window_size = config_dict.get('window_size', checkpoint.get('window_size', 200))
    finetune_timesfm = config_dict.get('finetune_timesfm', checkpoint.get('finetune_timesfm', True))
    
    # é‡æ–°åˆ›å»ºæŠ•èµ„ç»„åˆæ¨¡å‹
    portfolio_model = TimesFMPortfolioModelV2(
        input_size=input_size,
        output_size=input_size,
        feature_size=feature_size,
        timesfm_wrapper=timesfm_wrapper,
        context_len=window_size,
        finetune_timesfm=finetune_timesfm
    ).to(device)
    
    # åŠ è½½æ¨¡å‹çŠ¶æ€
    portfolio_model.load_state_dict(checkpoint['model_state_dict'])
    portfolio_model.eval()
    
    # é‡æ„é…ç½®å¯¹è±¡
    config = TrainingConfig()
    config.update(**config_dict)
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ:")
    print(f"  - æ¨¡å‹ç±»å‹: TimesFMæŠ•èµ„ç»„åˆæ¨¡å‹V2")
    print(f"  - è®­ç»ƒè½®æ•°: {checkpoint.get('epoch', 'æœªçŸ¥')}")
    print(f"  - éªŒè¯æŸå¤±: {checkpoint.get('val_loss', checkpoint.get('best_val_loss', 'æœªçŸ¥'))}")
    print(f"  - è‚¡ç¥¨æ•°é‡: {input_size}")
    print(f"  - ç‰¹å¾æ•°é‡: {feature_size}")
    print(f"  - æ—¶é—´çª—å£: {window_size}")
    print(f"  - TimesFMå¾®è°ƒ: {finetune_timesfm}")
    print(f"  - æ—¶é—´æˆ³: {checkpoint.get('timestamp', 'æœªçŸ¥')}")
    
    return portfolio_model, checkpoint, config


class ModelLoader:
    """æ¨¡å‹åŠ è½½å™¨ç±»"""
    
    def __init__(self, device='cpu'):
        self.device = device
    
    def load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        return torch.load(checkpoint_path, map_location=self.device, weights_only=False)
    
    def load_timesfm_portfolio_model(self, checkpoint_path):
        """åŠ è½½TimesFMæŠ•èµ„ç»„åˆæ¨¡å‹"""
        return load_timesfm_portfolio_model(checkpoint_path, self.device)
    
    def list_model_files(self, model_dir='models'):
        """åˆ—å‡ºæ¨¡å‹ç›®å½•ä¸­çš„æ‰€æœ‰æ¨¡å‹æ–‡ä»¶"""
        import glob
        
        if not os.path.exists(model_dir):
            print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
            return []
        
        model_files = glob.glob(os.path.join(model_dir, '*.pth'))
        
        if not model_files:
            print(f"ğŸ“ æ¨¡å‹ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_dir}")
            return []
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        model_files.sort(key=os.path.getmtime, reverse=True)
        
        print(f"ğŸ“ æ‰¾åˆ° {len(model_files)} ä¸ªæ¨¡å‹æ–‡ä»¶:")
        for i, model_file in enumerate(model_files):
            file_name = os.path.basename(model_file)
            file_size = os.path.getsize(model_file) / (1024 * 1024)  # MB
            mod_time = os.path.getmtime(model_file)
            mod_time_str = datetime.fromtimestamp(mod_time).strftime('%Y-%m-%d %H:%M:%S')
            
            model_type = "æœªçŸ¥"
            if "best" in file_name:
                model_type = "æœ€ä½³æ¨¡å‹"
            elif "final" in file_name:
                model_type = "æœ€ç»ˆæ¨¡å‹"
            elif "checkpoint" in file_name:
                model_type = "æ£€æŸ¥ç‚¹"
                
            print(f"  {i+1:2d}. {file_name}")
            print(f"      ç±»å‹: {model_type} | å¤§å°: {file_size:.1f}MB | æ—¶é—´: {mod_time_str}")
        
        return model_files
    
    def get_model_info(self, checkpoint_path):
        """è·å–æ¨¡å‹ä¿¡æ¯ï¼ˆä¸åŠ è½½æ¨¡å‹ï¼‰"""
        try:
            checkpoint = self.load_checkpoint(checkpoint_path)
            
            info = {
                'file_path': checkpoint_path,
                'file_name': os.path.basename(checkpoint_path),
                'epoch': checkpoint.get('epoch', 'æœªçŸ¥'),
                'val_loss': checkpoint.get('val_loss', checkpoint.get('best_val_loss', 'æœªçŸ¥')),
                'train_loss': checkpoint.get('train_loss', checkpoint.get('final_train_loss', 'æœªçŸ¥')),
                'timestamp': checkpoint.get('timestamp', 'æœªçŸ¥'),
                'config': checkpoint.get('config', {}),
                'model_params': {
                    'input_size': checkpoint.get('input_size', checkpoint.get('config', {}).get('input_size', 'æœªçŸ¥')),
                    'feature_size': checkpoint.get('feature_size', checkpoint.get('config', {}).get('feature_size', 'æœªçŸ¥')),
                    'window_size': checkpoint.get('window_size', checkpoint.get('config', {}).get('window_size', 'æœªçŸ¥')),
                    'finetune_timesfm': checkpoint.get('finetune_timesfm', checkpoint.get('config', {}).get('finetune_timesfm', 'æœªçŸ¥'))
                }
            }
            
            return info
            
        except Exception as e:
            print(f"âŒ è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def compare_models(self, model_paths):
        """æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„ä¿¡æ¯"""
        print("ğŸ” æ¨¡å‹å¯¹æ¯”:")
        print("-" * 100)
        print(f"{'æ–‡ä»¶å':<40} {'è½®æ•°':<6} {'éªŒè¯æŸå¤±':<12} {'è®­ç»ƒæŸå¤±':<12} {'æ—¶é—´æˆ³':<20}")
        print("-" * 100)
        
        for path in model_paths:
            info = self.get_model_info(path)
            if info:
                file_name = info['file_name'][:37] + "..." if len(info['file_name']) > 40 else info['file_name']
                val_loss = f"{info['val_loss']:.6f}" if isinstance(info['val_loss'], (int, float)) else str(info['val_loss'])
                train_loss = f"{info['train_loss']:.6f}" if isinstance(info['train_loss'], (int, float)) else str(info['train_loss'])
                
                print(f"{file_name:<40} {info['epoch']:<6} {val_loss:<12} {train_loss:<12} {info['timestamp']:<20}")
        
        print("-" * 100)