"""
æ•°æ®å¤„ç†æ¨¡å—

å¤„ç†æ—¶é—´åºåˆ—æ•°æ®ï¼ŒåŒ…æ‹¬åŠ è½½ã€æ¸…ç†ã€ç‰¹å¾å·¥ç¨‹å’Œæ ¼å¼è½¬æ¢
"""

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from typing import Tuple, Dict, List, Optional, Any
from sklearn.preprocessing import StandardScaler
import warnings


class MAG7DataProcessor:
    """MAG7è‚¡ç¥¨æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, asset_names: List[str] = None):
        self.asset_names = asset_names or [
            'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'TSLA', 'META'
        ]
        self.scaler = StandardScaler()
        
    def load_data(self, data_path: str) -> pd.DataFrame:
        """åŠ è½½MAG7æ•°æ®"""
        print(f"ğŸ“¥ åŠ è½½æ•°æ®: {data_path}")
        
        if data_path.endswith('.parquet'):
            df = pd.read_parquet(data_path)
        elif data_path.endswith('.csv'):
            df = pd.read_csv(data_path, index_col=0, parse_dates=True)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {data_path}")
            
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {df.shape}")
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: {df.index.min()} åˆ° {df.index.max()}")
        return df

    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®æ¸…ç†"""
        print("ğŸ§¹ å¼€å§‹æ•°æ®æ¸…ç†...")
        
        # ç§»é™¤ç¼ºå¤±å€¼è¿‡å¤šçš„è¡Œ
        threshold = len(self.asset_names) * 0.8  # è‡³å°‘80%çš„èµ„äº§æœ‰æ•°æ®
        df_clean = df.dropna(thresh=threshold)
        
        # å‰å‘å¡«å……ç¼ºå¤±å€¼
        df_clean = df_clean.fillna(method='ffill')
        
        # ç§»é™¤ä»æœ‰ç¼ºå¤±å€¼çš„è¡Œ
        df_clean = df_clean.dropna()
        
        print(f"âœ… æ•°æ®æ¸…ç†å®Œæˆ: {df.shape} â†’ {df_clean.shape}")
        return df_clean

    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾å·¥ç¨‹"""
        print("âš¡ å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
        
        # ä¸ºæ¯ä¸ªèµ„äº§è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        feature_df_list = []
        
        for asset in self.asset_names:
            if asset not in df.columns:
                print(f"âš ï¸ èµ„äº§ {asset} ä¸åœ¨æ•°æ®ä¸­ï¼Œè·³è¿‡")
                continue
                
            prices = df[asset]
            
            # è®¡ç®—å„ç§æŠ€æœ¯æŒ‡æ ‡
            asset_features = pd.DataFrame(index=df.index)
            
            # åŸºç¡€ç‰¹å¾
            asset_features[f'{asset}_price'] = prices
            asset_features[f'{asset}_returns'] = prices.pct_change()
            asset_features[f'{asset}_log_returns'] = np.log(prices / prices.shift(1))
            
            # æ ‡å‡†åŒ–ä»·æ ¼å’Œæˆäº¤é‡ (å¦‚æœæœ‰çš„è¯)
            asset_features[f'{asset}_price_norm'] = (prices - prices.rolling(60).mean()) / prices.rolling(60).std()
            
            # å¦‚æœæœ‰æˆäº¤é‡æ•°æ®
            volume_col = f'{asset}_volume'
            if volume_col in df.columns:
                volume = df[volume_col]
                asset_features[f'{asset}_volume_norm'] = (volume - volume.rolling(60).mean()) / volume.rolling(60).std()
            else:
                # å¦‚æœæ²¡æœ‰æˆäº¤é‡æ•°æ®ï¼Œä½¿ç”¨ä»·æ ¼å˜åŒ–çš„ç»å¯¹å€¼ä½œä¸ºæ›¿ä»£
                asset_features[f'{asset}_volume_norm'] = asset_features[f'{asset}_returns'].abs()
            
            feature_df_list.append(asset_features)
        
        # åˆå¹¶æ‰€æœ‰èµ„äº§çš„ç‰¹å¾
        features_df = pd.concat(feature_df_list, axis=1)
        
        # ç§»é™¤å‰60è¡Œï¼ˆå› ä¸ºæ»šåŠ¨è®¡ç®—ï¼‰
        features_df = features_df.iloc[60:].copy()
        
        print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ: {features_df.shape}")
        return features_df

    def to_deepdow_format(
        self, 
        features_df: pd.DataFrame, 
        sequence_length: int = 60, 
        prediction_horizon: int = 1
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """è½¬æ¢ä¸ºdeepdowæ ¼å¼"""
        print(f"ğŸ“Š è½¬æ¢ä¸ºdeepdowæ ¼å¼ (åºåˆ—é•¿åº¦: {sequence_length}, é¢„æµ‹èŒƒå›´: {prediction_horizon})...")
        
        # é‡æ–°ç»„ç»‡æ•°æ®ä¸º (æ ·æœ¬, æŒ‡æ ‡, æ—¶é—´çª—å£, èµ„äº§) æ ¼å¼
        n_assets = len(self.asset_names)
        n_indicators = 4  # returns, log_returns, price_norm, volume_norm
        
        # åˆ›å»ºæŒ‡æ ‡åˆ—è¡¨
        indicators = ['returns', 'log_returns', 'price_norm', 'volume_norm']
        
        # æå–æ¯ä¸ªèµ„äº§çš„å„ä¸ªæŒ‡æ ‡æ•°æ®
        asset_data = {}
        for asset in self.asset_names:
            asset_data[asset] = {}
            for indicator in indicators:
                col_name = f'{asset}_{indicator}'
                if col_name in features_df.columns:
                    asset_data[asset][indicator] = features_df[col_name].values
                else:
                    print(f"âš ï¸ åˆ— {col_name} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é›¶å¡«å……")
                    asset_data[asset][indicator] = np.zeros(len(features_df))
        
        # åˆ›å»ºæ»‘åŠ¨çª—å£æ ·æœ¬
        n_samples = len(features_df) - sequence_length - prediction_horizon + 1
        
        X = np.zeros((n_samples, n_indicators, sequence_length, n_assets))
        y = np.zeros((n_samples, n_assets, prediction_horizon))
        
        for i in range(n_samples):
            for asset_idx, asset in enumerate(self.asset_names):
                for indicator_idx, indicator in enumerate(indicators):
                    # ç‰¹å¾çª—å£
                    X[i, indicator_idx, :, asset_idx] = asset_data[asset][indicator][i:i+sequence_length]
                
                # æœªæ¥æ”¶ç›Šç‡ (ä½¿ç”¨returnsä½œä¸ºç›®æ ‡)
                future_returns = asset_data[asset]['returns'][i+sequence_length:i+sequence_length+prediction_horizon]
                y[i, asset_idx, :] = future_returns
        
        # è½¬æ¢ä¸ºå¼ é‡
        X_tensor = torch.FloatTensor(X)
        y_tensor = torch.FloatTensor(y)
        
        print(f"âœ… æ ¼å¼è½¬æ¢å®Œæˆ:")
        print(f"  - ç‰¹å¾å½¢çŠ¶: {X_tensor.shape} (æ ·æœ¬, æŒ‡æ ‡, æ—¶é—´çª—å£, èµ„äº§)")
        print(f"  - ç›®æ ‡å½¢çŠ¶: {y_tensor.shape} (æ ·æœ¬, èµ„äº§, é¢„æµ‹èŒƒå›´)")
        
        return X_tensor, y_tensor

    def create_data_splits(
        self, 
        X: torch.Tensor, 
        y: torch.Tensor, 
        train_ratio: float = 0.7, 
        val_ratio: float = 0.15
    ) -> Tuple[Tuple[torch.Tensor, torch.Tensor], ...]:
        """åˆ›å»ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®åˆ†å‰²"""
        n_samples = X.shape[0]
        
        train_end = int(n_samples * train_ratio)
        val_end = int(n_samples * (train_ratio + val_ratio))
        
        # åˆ†å‰²æ•°æ®
        X_train, y_train = X[:train_end], y[:train_end]
        X_val, y_val = X[train_end:val_end], y[train_end:val_end]
        X_test, y_test = X[val_end:], y[val_end:]
        
        print(f"ğŸ“Š æ•°æ®åˆ†å‰²å®Œæˆ:")
        print(f"  - è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬ ({train_ratio*100:.1f}%)")
        print(f"  - éªŒè¯é›†: {X_val.shape[0]} æ ·æœ¬ ({val_ratio*100:.1f}%)")
        print(f"  - æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬ ({(1-train_ratio-val_ratio)*100:.1f}%)")
        
        return (X_train, y_train), (X_val, y_val), (X_test, y_test)


class PortfolioDataset(Dataset):
    """æŠ•èµ„ç»„åˆæ•°æ®é›†"""
    
    def __init__(self, features: torch.Tensor, targets: torch.Tensor):
        self.features = features
        self.targets = targets
        
    def __len__(self):
        return self.features.shape[0]
        
    def __getitem__(self, idx):
        return self.features[idx], self.targets[idx]


def create_data_loaders(
    train_data: Tuple[torch.Tensor, torch.Tensor],
    val_data: Tuple[torch.Tensor, torch.Tensor], 
    test_data: Tuple[torch.Tensor, torch.Tensor],
    batch_size: int = 32,
    num_workers: int = 0
) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = PortfolioDataset(*train_data)
    val_dataset = PortfolioDataset(*val_data)
    test_dataset = PortfolioDataset(*test_data)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=num_workers,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=True
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
    print(f"  - è®­ç»ƒ: {len(train_loader)} æ‰¹æ¬¡")
    print(f"  - éªŒè¯: {len(val_loader)} æ‰¹æ¬¡") 
    print(f"  - æµ‹è¯•: {len(test_loader)} æ‰¹æ¬¡")
    
    return train_loader, val_loader, test_loader


def calculate_benchmark_portfolios(
    returns_data: pd.DataFrame, 
    asset_names: List[str]
) -> Dict[str, Dict[str, Any]]:
    """è®¡ç®—åŸºå‡†æŠ•èµ„ç»„åˆç­–ç•¥"""
    print("âš–ï¸ è®¡ç®—åŸºå‡†ç­–ç•¥æ€§èƒ½...")
    
    # æå–æ”¶ç›Šç‡çŸ©é˜µ
    if isinstance(returns_data.columns, pd.MultiIndex):
        # å¤šçº§åˆ—åæƒ…å†µ
        returns_columns = []
        for asset in asset_names:
            for col in [(asset, 'returns'), (asset, 'log_returns')]:
                if col in returns_data.columns:
                    returns_columns.append(col)
                    break
        
        if returns_columns:
            returns_df = returns_data[returns_columns]
            returns_df.columns = [col[0] for col in returns_columns]
        else:
            raise ValueError("æ— æ³•æ‰¾åˆ°æ”¶ç›Šç‡åˆ—")
    else:
        # å•çº§åˆ—åæƒ…å†µ
        returns_cols = [col for col in returns_data.columns if 'returns' in col and any(asset in col for asset in asset_names)]
        returns_df = returns_data[returns_cols[:len(asset_names)]]
        returns_df.columns = asset_names[:len(returns_cols)]
    
    returns_matrix = returns_df.values
    
    benchmarks = {}
    
    # 1. ç­‰æƒé‡ç­–ç•¥
    equal_weights = np.ones(len(asset_names)) / len(asset_names)
    equal_portfolio_returns = returns_matrix @ equal_weights
    benchmarks['ç­‰æƒé‡'] = {
        'weights': equal_weights,
        'returns': equal_portfolio_returns,
        'name': 'ç­‰æƒé‡ç­–ç•¥'
    }
    
    # 2. æ³¢åŠ¨ç‡å€’æ•°åŠ æƒç­–ç•¥
    try:
        returns_std = returns_matrix.std(axis=0)
        returns_std = np.where(returns_std == 0, 1e-8, returns_std)
        vol_weights = 1.0 / returns_std
        vol_weights = vol_weights / vol_weights.sum()
        
        vol_portfolio_returns = returns_matrix @ vol_weights
        benchmarks['æ³¢åŠ¨ç‡å€’æ•°åŠ æƒ'] = {
            'weights': vol_weights,
            'returns': vol_portfolio_returns,
            'name': 'æ³¢åŠ¨ç‡å€’æ•°åŠ æƒç­–ç•¥'
        }
    except Exception as e:
        print(f"âš ï¸ æ³¢åŠ¨ç‡å€’æ•°åŠ æƒç­–ç•¥å¤±è´¥: {e}")
        benchmarks['æ³¢åŠ¨ç‡å€’æ•°åŠ æƒ'] = benchmarks['ç­‰æƒé‡'].copy()
    
    # 3. æœ€å°æ–¹å·®ç­–ç•¥
    try:
        cov_matrix = np.cov(returns_matrix.T)
        eigenvals = np.linalg.eigvals(cov_matrix)
        if np.min(eigenvals) <= 0:
            cov_matrix += np.eye(cov_matrix.shape[0]) * 1e-8
            
        inv_vol = 1.0 / np.sqrt(np.diag(cov_matrix))
        min_var_weights = inv_vol / inv_vol.sum()
        
        min_var_portfolio_returns = returns_matrix @ min_var_weights
        benchmarks['æœ€å°æ–¹å·®'] = {
            'weights': min_var_weights,
            'returns': min_var_portfolio_returns,
            'name': 'æœ€å°æ–¹å·®ç­–ç•¥'
        }
    except Exception as e:
        print(f"âš ï¸ æœ€å°æ–¹å·®ç­–ç•¥å¤±è´¥: {e}")
        benchmarks['æœ€å°æ–¹å·®'] = benchmarks['ç­‰æƒé‡'].copy()
    
    # 4. æœ€ä½³èµ„äº§ç­–ç•¥
    try:
        mean_returns = returns_matrix.mean(axis=0)
        best_asset_idx = np.argmax(mean_returns)
        best_asset_name = asset_names[best_asset_idx]
        
        best_asset_weights = np.zeros(len(asset_names))
        best_asset_weights[best_asset_idx] = 1.0
        
        best_asset_portfolio_returns = returns_matrix @ best_asset_weights
        
        benchmarks['æœ€ä½³èµ„äº§'] = {
            'weights': best_asset_weights,
            'returns': best_asset_portfolio_returns,
            'name': f'æœ€ä½³èµ„äº§ç­–ç•¥ ({best_asset_name})'
        }
    except Exception as e:
        print(f"âš ï¸ æœ€ä½³èµ„äº§ç­–ç•¥å¤±è´¥: {e}")
        benchmarks['æœ€ä½³èµ„äº§'] = benchmarks['ç­‰æƒé‡'].copy()
    
    print(f"âœ… æˆåŠŸè®¡ç®— {len(benchmarks)} ä¸ªåŸºå‡†ç­–ç•¥")
    return benchmarks